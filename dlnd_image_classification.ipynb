{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Image Classification\n",
    "In this project, you'll classify images from the [CIFAR-10 dataset](https://www.cs.toronto.edu/~kriz/cifar.html).  The dataset consists of airplanes, dogs, cats, and other objects. You'll preprocess the images, then train a convolutional neural network on all the samples. The images need to be normalized and the labels need to be one-hot encoded.  You'll get to apply what you learned and build a convolutional, max pooling, dropout, and fully connected layers.  At the end, you'll get to see your neural network's predictions on the sample images.\n",
    "## Get the Data\n",
    "Run the following cell to download the [CIFAR-10 dataset for python](https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All files found!\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "from urllib.request import urlretrieve\n",
    "from os.path import isfile, isdir\n",
    "from tqdm import tqdm\n",
    "import problem_unittests as tests\n",
    "import tarfile\n",
    "\n",
    "cifar10_dataset_folder_path = 'cifar-10-batches-py'\n",
    "\n",
    "class DLProgress(tqdm):\n",
    "    last_block = 0\n",
    "\n",
    "    def hook(self, block_num=1, block_size=1, total_size=None):\n",
    "        self.total = total_size\n",
    "        self.update((block_num - self.last_block) * block_size)\n",
    "        self.last_block = block_num\n",
    "\n",
    "if not isfile('cifar-10-python.tar.gz'):\n",
    "    with DLProgress(unit='B', unit_scale=True, miniters=1, desc='CIFAR-10 Dataset') as pbar:\n",
    "        urlretrieve(\n",
    "            'https://www.cs.toronto.edu/~kriz/cifar-10-python.tar.gz',\n",
    "            'cifar-10-python.tar.gz',\n",
    "            pbar.hook)\n",
    "\n",
    "if not isdir(cifar10_dataset_folder_path):\n",
    "    with tarfile.open('cifar-10-python.tar.gz') as tar:\n",
    "        tar.extractall()\n",
    "        tar.close()\n",
    "\n",
    "\n",
    "tests.test_folder_path(cifar10_dataset_folder_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Explore the Data\n",
    "The dataset is broken into batches to prevent your machine from running out of memory.  The CIFAR-10 dataset consists of 5 batches, named `data_batch_1`, `data_batch_2`, etc.. Each batch contains the labels and images that are one of the following:\n",
    "* airplane\n",
    "* automobile\n",
    "* bird\n",
    "* cat\n",
    "* deer\n",
    "* dog\n",
    "* frog\n",
    "* horse\n",
    "* ship\n",
    "* truck\n",
    "\n",
    "Understanding a dataset is part of making predictions on the data.  Play around with the code cell below by changing the `batch_id` and `sample_id`. The `batch_id` is the id for a batch (1-5). The `sample_id` is the id for a image and label pair in the batch.\n",
    "\n",
    "Ask yourself \"What are all possible labels?\", \"What is the range of values for the image data?\", \"Are the labels in order or random?\".  Answers to questions like these will help you preprocess the data and end up with better predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Stats of batch 1:\n",
      "Samples: 10000\n",
      "Label Counts: {0: 1005, 1: 974, 2: 1032, 3: 1016, 4: 999, 5: 937, 6: 1030, 7: 1001, 8: 1025, 9: 981}\n",
      "First 20 Labels: [6, 9, 9, 4, 1, 1, 2, 7, 8, 3, 4, 7, 7, 2, 9, 9, 9, 3, 2, 6]\n",
      "\n",
      "Example of Image 5:\n",
      "Image - Min Value: 0 Max Value: 252\n",
      "Image - Shape: (32, 32, 3)\n",
      "Label - Label Id: 1 Name: automobile\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAfoAAAH0CAYAAADVH+85AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nrkJggg==\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x13566134f60>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 250,
       "width": 253
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import helper\n",
    "import numpy as np\n",
    "\n",
    "# Explore the dataset\n",
    "batch_id = 1\n",
    "sample_id = 5\n",
    "helper.display_stats(cifar10_dataset_folder_path, batch_id, sample_id)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Implement Preprocess Functions\n",
    "### Normalize\n",
    "In the cell below, implement the `normalize` function to take in image data, `x`, and return it as a normalized Numpy array. The values should be in the range of 0 to 1, inclusive.  The return object should be the same shape as `x`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def normalize(x):\n",
    "    \"\"\"\n",
    "    Normalize a list of sample image data in the range of 0 to 1\n",
    "    : x: List of image data.  The image shape is (32, 32, 3)\n",
    "    : return: Numpy array of normalize data\n",
    "    \"\"\"\n",
    "   \n",
    "    result = np.copy(x).astype(np.float32)\n",
    "    \n",
    "    for a in np.nditer(result, op_flags=['readwrite']):\n",
    "          a[...] =  a / 255\n",
    "           \n",
    "    return result\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_normalize(normalize)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### One-hot encode\n",
    "Just like the previous code cell, you'll be implementing a function for preprocessing.  This time, you'll implement the `one_hot_encode` function. The input, `x`, are a list of labels.  Implement the function to return the list of labels as One-Hot encoded Numpy array.  The possible values for labels are 0 to 9. The one-hot encoding function should return the same encoding for each value between each call to `one_hot_encode`.  Make sure to save the map of encodings outside the function.\n",
    "\n",
    "Hint: Don't reinvent the wheel."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "arrayOfpossibleLabelValues = [0,1,2,3,4,5,6,7,8,9]\n",
    "\n",
    "def one_hot_encode(x):\n",
    "    \"\"\"\n",
    "    One hot encode a list of sample labels. Return a one-hot encoded vector for each label.\n",
    "    : x: List of sample Labels\n",
    "    : return: Numpy array of one-hot encoded labels\n",
    "    \"\"\"\n",
    "    lb = preprocessing.LabelBinarizer()\n",
    "    lb.fit(arrayOfpossibleLabelValues)    \n",
    "    \n",
    "    return lb.transform(x)\n",
    "    \n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_one_hot_encode(one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Randomize Data\n",
    "As you saw from exploring the data above, the order of the samples are randomized.  It doesn't hurt to randomize it again, but you don't need to for this dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess all the data and save it\n",
    "Running the code cell below will preprocess all the CIFAR-10 data and save it to file. The code below also uses 10% of the training data for validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "# Preprocess Training, Validation, and Testing Data\n",
    "helper.preprocess_and_save_data(cifar10_dataset_folder_path, normalize, one_hot_encode)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Check Point\n",
    "This is your first checkpoint.  If you ever decide to come back to this notebook or have to restart the notebook, you can start from here.  The preprocessed data has been saved to disk."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "import pickle\n",
    "import problem_unittests as tests\n",
    "import helper\n",
    "\n",
    "# Load the Preprocessed Validation data\n",
    "valid_features, valid_labels = pickle.load(open('preprocess_validation.p', mode='rb'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Build the network\n",
    "For the neural network, you'll build each layer into a function.  Most of the code you've seen has been outside of functions. To test your code more thoroughly, we require that you put each layer in a function.  This allows us to give you better feedback and test for simple mistakes using our unittests before you submit your project.\n",
    "\n",
    ">**Note:** If you're finding it hard to dedicate enough time for this course each week, we've provided a small shortcut to this part of the project. In the next couple of problems, you'll have the option to use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages to build each layer, except the layers you build in the \"Convolutional and Max Pooling Layer\" section.  TF Layers is similar to Keras's and TFLearn's abstraction to layers, so it's easy to pickup.\n",
    "\n",
    ">However, if you would like to get the most out of this course, try to solve all the problems _without_ using anything from the TF Layers packages. You **can** still use classes from other packages that happen to have the same name as ones you find in TF Layers! For example, instead of using the TF Layers version of the `conv2d` class, [tf.layers.conv2d](https://www.tensorflow.org/api_docs/python/tf/layers/conv2d), you would want to use the TF Neural Network version of `conv2d`, [tf.nn.conv2d](https://www.tensorflow.org/api_docs/python/tf/nn/conv2d). \n",
    "\n",
    "Let's begin!\n",
    "\n",
    "### Input\n",
    "The neural network needs to read the image data, one-hot encoded labels, and dropout keep probability. Implement the following functions\n",
    "* Implement `neural_net_image_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `image_shape` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"x\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_label_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder)\n",
    " * Set the shape using `n_classes` with batch size set to `None`.\n",
    " * Name the TensorFlow placeholder \"y\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "* Implement `neural_net_keep_prob_input`\n",
    " * Return a [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder) for dropout keep probability.\n",
    " * Name the TensorFlow placeholder \"keep_prob\" using the TensorFlow `name` parameter in the [TF Placeholder](https://www.tensorflow.org/api_docs/python/tf/placeholder).\n",
    "\n",
    "These names will be used at the end of the project to load your saved model.\n",
    "\n",
    "Note: `None` for shapes in TensorFlow allow for a dynamic size."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Image Input Tests Passed.\n",
      "Label Input Tests Passed.\n",
      "Keep Prob Tests Passed.\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "\n",
    "\n",
    "def neural_net_image_input(image_shape):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a bach of image input\n",
    "    : image_shape: Shape of the images\n",
    "    : return: Tensor for image input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, shape = [None, *image_shape], name='x')\n",
    "\n",
    "\n",
    "def neural_net_label_input(n_classes):\n",
    "    \"\"\"\n",
    "    Return a Tensor for a batch of label input\n",
    "    : n_classes: Number of classes\n",
    "    : return: Tensor for label input.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32, shape = [None, n_classes] ,name='y')\n",
    "\n",
    "\n",
    "def neural_net_keep_prob_input():\n",
    "    \"\"\"\n",
    "    Return a Tensor for keep probability\n",
    "    : return: Tensor for keep probability.\n",
    "    \"\"\"\n",
    "    return tf.placeholder(tf.float32,name='keep_prob')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tf.reset_default_graph()\n",
    "tests.test_nn_image_inputs(neural_net_image_input)\n",
    "tests.test_nn_label_inputs(neural_net_label_input)\n",
    "tests.test_nn_keep_prob_inputs(neural_net_keep_prob_input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convolution and Max Pooling Layer\n",
    "Convolution layers have a lot of success with images. For this code cell, you should implement the function `conv2d_maxpool` to apply convolution then max pooling:\n",
    "* Create the weight and bias using `conv_ksize`, `conv_num_outputs` and the shape of `x_tensor`.\n",
    "* Apply a convolution to `x_tensor` using weight and `conv_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "* Add bias\n",
    "* Add a nonlinear activation to the convolution.\n",
    "* Apply Max Pooling using `pool_ksize` and `pool_strides`.\n",
    " * We recommend you use same padding, but you're welcome to use any padding.\n",
    "\n",
    "**Note:** You **can't** use [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) for **this** layer, but you can still use TensorFlow's [Neural Network](https://www.tensorflow.org/api_docs/python/tf/nn) package. You may still use the shortcut option for all the **other** layers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides):\n",
    "    \"\"\"\n",
    "    Apply convolution then max pooling to x_tensor\n",
    "    :param x_tensor: TensorFlow Tensor\n",
    "    :param conv_num_outputs: Number of outputs for the convolutional layer\n",
    "    :param conv_ksize: kernal size 2-D Tuple for the convolutional layer\n",
    "    :param conv_strides: Stride 2-D Tuple for convolution\n",
    "    :param pool_ksize: kernal size 2-D Tuple for pool\n",
    "    :param pool_strides: Stride 2-D Tuple for pool\n",
    "    : return: A tensor that represents convolution and max pooling of x_tensor\n",
    "    \"\"\"   \n",
    "    filter_weights = tf.Variable(tf.truncated_normal([conv_ksize[0],conv_ksize[1], x_tensor.get_shape().as_list()[3],conv_num_outputs], stddev=0.05)) \n",
    "    filter_bias = tf.Variable(tf.zeros(conv_num_outputs))\n",
    "        \n",
    "    conv_layer = tf.nn.conv2d(x_tensor, filter_weights, strides= [1,conv_strides[0],conv_strides[1],1], padding='SAME')\n",
    "    conv_layer = tf.nn.bias_add(conv_layer, filter_bias)\n",
    "    conv_layer = tf.nn.relu(conv_layer)\n",
    "    return tf.nn.max_pool(conv_layer, ksize=[1, pool_ksize[0], pool_ksize[1], 1],  strides = [1, pool_strides[0], pool_strides[1], 1], padding='SAME')\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_con_pool(conv2d_maxpool)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Flatten Layer\n",
    "Implement the `flatten` function to change the dimension of `x_tensor` from a 4-D tensor to a 2-D tensor.  The output should be the shape (*Batch Size*, *Flattened Image Size*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def flatten(x_tensor):\n",
    "    \"\"\"\n",
    "    Flatten x_tensor to (Batch Size, Flattened Image Size)\n",
    "    : x_tensor: A tensor of size (Batch Size, ...), where ... are the image dimensions.\n",
    "    : return: A tensor of size (Batch Size, Flattened Image Size).\n",
    "    \"\"\"\n",
    "    return tf.reshape(x_tensor, [-1, np.prod(x_tensor.get_shape().as_list()[1:])])\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_flatten(flatten)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully-Connected Layer\n",
    "Implement the `fully_conn` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def fully_conn(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a fully connected layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    weights = tf.Variable(tf.truncated_normal([x_tensor.get_shape().as_list()[1], num_outputs], mean=0.0, stddev=0.05))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    return tf.nn.relu(tf.add(tf.matmul(x_tensor, weights), bias))\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_fully_conn(fully_conn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Output Layer\n",
    "Implement the `output` function to apply a fully connected layer to `x_tensor` with the shape (*Batch Size*, *num_outputs*). Shortcut option: you can use classes from the [TensorFlow Layers](https://www.tensorflow.org/api_docs/python/tf/layers) or [TensorFlow Layers (contrib)](https://www.tensorflow.org/api_guides/python/contrib.layers) packages for this layer. For more of a challenge, only use other TensorFlow packages.\n",
    "\n",
    "**Note:** Activation, softmax, or cross entropy should **not** be applied to this."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def output(x_tensor, num_outputs):\n",
    "    \"\"\"\n",
    "    Apply a output layer to x_tensor using weight and bias\n",
    "    : x_tensor: A 2-D tensor where the first dimension is batch size.\n",
    "    : num_outputs: The number of output that the new tensor should be.\n",
    "    : return: A 2-D tensor where the second dimension is num_outputs.\n",
    "    \"\"\"\n",
    "    weights = tf.Variable(tf.truncated_normal([x_tensor.get_shape().as_list()[1], num_outputs], mean=0.0, stddev=0.05))\n",
    "    bias = tf.Variable(tf.zeros(num_outputs))\n",
    "    \n",
    "    return tf.add(tf.matmul(x_tensor, weights), bias)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_output(output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Convolutional Model\n",
    "Implement the function `conv_net` to create a convolutional neural network model. The function takes in a batch of images, `x`, and outputs logits.  Use the layers you created above to create this model:\n",
    "\n",
    "* Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "* Apply a Flatten Layer\n",
    "* Apply 1, 2, or 3 Fully Connected Layers\n",
    "* Apply an Output Layer\n",
    "* Return the output\n",
    "* Apply [TensorFlow's Dropout](https://www.tensorflow.org/api_docs/python/tf/nn/dropout) to one or more layers in the model using `keep_prob`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Neural Network Built!\n"
     ]
    }
   ],
   "source": [
    "def conv_net(x, keep_prob):\n",
    "    \"\"\"\n",
    "    Create a convolutional neural network model\n",
    "    : x: Placeholder tensor that holds image data.\n",
    "    : keep_prob: Placeholder tensor that hold dropout keep probability.\n",
    "    : return: Tensor that represents logits\n",
    "    \"\"\"\n",
    "    \n",
    "    conv_num_outputs = [32,128,512]\n",
    "    conv_ksize = [(8,8),(4,4),(2,2)]\n",
    "    conv_strides = (1,1)\n",
    "    pool_ksize = (4,4)\n",
    "    pool_strides = (2,2)\n",
    "    num_outputs = 10\n",
    "        \n",
    "    # TODO: Apply 1, 2, or 3 Convolution and Max Pool layers\n",
    "    #    Play around with different number of outputs, kernel size and stride\n",
    "    # Function Definition from Above:\n",
    "    #    conv2d_maxpool(x_tensor, conv_num_outputs, conv_ksize, conv_strides, pool_ksize, pool_strides)\n",
    "    net = conv2d_maxpool(x, conv_num_outputs[0],conv_ksize[0],conv_strides,pool_ksize,pool_strides)    \n",
    "    net = conv2d_maxpool(net, conv_num_outputs[1],conv_ksize[1],conv_strides,pool_ksize,pool_strides)\n",
    "    net = conv2d_maxpool(net, conv_num_outputs[2],conv_ksize[2],conv_strides,pool_ksize,pool_strides)\n",
    "        \n",
    "    # TODO: Apply a Flatten Layer\n",
    "    # Function Definition from Above:\n",
    "    #   flatten(x_tensor)\n",
    "    net = flatten(net)\n",
    "\n",
    "    # TODO: Apply 1, 2, or 3 Fully Connected Layers\n",
    "    #    Play around with different number of outputs\n",
    "    # Function Definition from Above:\n",
    "    #   fully_conn(x_tensor, num_outputs)\n",
    "    net = fully_conn(net,512)\n",
    "    net = tf.nn.dropout(net, keep_prob)\n",
    "    net = fully_conn(net,128)\n",
    "    net = tf.nn.dropout(net, keep_prob)\n",
    "    net = fully_conn(net,32)\n",
    "    \n",
    "    # TODO: Apply an Output Layer\n",
    "    #    Set this to the number of classes\n",
    "    # Function Definition from Above:\n",
    "    #   output(x_tensor, num_outputs)\n",
    "    \n",
    "    return output(net,num_outputs)\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "\n",
    "##############################\n",
    "## Build the Neural Network ##\n",
    "##############################\n",
    "\n",
    "# Remove previous weights, bias, inputs, etc..\n",
    "tf.reset_default_graph()\n",
    "\n",
    "# Inputs\n",
    "x = neural_net_image_input((32, 32, 3))\n",
    "y = neural_net_label_input(10)\n",
    "keep_prob = neural_net_keep_prob_input()\n",
    "\n",
    "# Model\n",
    "logits = conv_net(x, keep_prob)\n",
    "\n",
    "# Name logits Tensor, so that is can be loaded from disk after training\n",
    "logits = tf.identity(logits, name='logits')\n",
    "\n",
    "# Loss and Optimizer\n",
    "cost = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(logits=logits, labels=y))\n",
    "optimizer = tf.train.AdamOptimizer().minimize(cost)\n",
    "\n",
    "# Accuracy\n",
    "correct_pred = tf.equal(tf.argmax(logits, 1), tf.argmax(y, 1))\n",
    "accuracy = tf.reduce_mean(tf.cast(correct_pred, tf.float32), name='accuracy')\n",
    "\n",
    "tests.test_conv_net(conv_net)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train the Neural Network\n",
    "### Single Optimization\n",
    "Implement the function `train_neural_network` to do a single optimization.  The optimization should use `optimizer` to optimize in `session` with a `feed_dict` of the following:\n",
    "* `x` for image input\n",
    "* `y` for labels\n",
    "* `keep_prob` for keep probability for dropout\n",
    "\n",
    "This function will be called for each batch, so `tf.global_variables_initializer()` has already been called.\n",
    "\n",
    "Note: Nothing needs to be returned. This function is only optimizing the neural network."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tests Passed\n"
     ]
    }
   ],
   "source": [
    "def train_neural_network(session, optimizer, keep_probability, feature_batch, label_batch):\n",
    "    \"\"\"\n",
    "    Optimize the session on a batch of images and labels\n",
    "    : session: Current TensorFlow session\n",
    "    : optimizer: TensorFlow optimizer function\n",
    "    : keep_probability: keep probability\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    \"\"\"\n",
    "    session.run(optimizer,feed_dict={x: feature_batch, y: label_batch, keep_prob: keep_probability})\n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL THAT IS BELOW THIS LINE\n",
    "\"\"\"\n",
    "tests.test_train_nn(train_neural_network)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Show Stats\n",
    "Implement the function `print_stats` to print loss and validation accuracy.  Use the global variables `valid_features` and `valid_labels` to calculate validation accuracy.  Use a keep probability of `1.0` to calculate the loss and validation accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def print_stats(session, feature_batch, label_batch, cost, accuracy):\n",
    "    \"\"\"\n",
    "    Print information about loss and validation accuracy\n",
    "    : session: Current TensorFlow session\n",
    "    : feature_batch: Batch of Numpy image data\n",
    "    : label_batch: Batch of Numpy label data\n",
    "    : cost: TensorFlow cost function\n",
    "    : accuracy: TensorFlow accuracy function\n",
    "    \"\"\"\n",
    "    loss = session.run(cost, feed_dict={x: feature_batch, y: label_batch, keep_prob: 1.0})\n",
    "    accuracy = session.run(accuracy, feed_dict={x: valid_features, y: valid_labels, keep_prob: 1.0})\n",
    "    print(\"Loss: {}\".format(loss))\n",
    "    print(\"Accuracy: {}\".format(accuracy))\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hyperparameters\n",
    "Tune the following parameters:\n",
    "* Set `epochs` to the number of iterations until the network stops learning or start overfitting\n",
    "* Set `batch_size` to the highest number that your machine has memory for.  Most people set them to common sizes of memory:\n",
    " * 64\n",
    " * 128\n",
    " * 256\n",
    " * ...\n",
    "* Set `keep_probability` to the probability of keeping a node using dropout"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TODO: Tune Parameters\n",
    "epochs = 200\n",
    "batch_size = 512\n",
    "keep_probability = 0.5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train on a Single CIFAR-10 Batch\n",
    "Instead of training the neural network on all the CIFAR-10 batches of data, let's use a single batch. This should save time while you iterate on the model to get a better accuracy.  Once the final validation accuracy is 50% or greater, run the model on all the data in the next section."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Checking the Training on a Single Batch...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss: 2.2927119731903076\n",
      "Accuracy: 0.12540000677108765\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss: 2.2804012298583984\n",
      "Accuracy: 0.11779999732971191\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss: 2.180907726287842\n",
      "Accuracy: 0.17799998819828033\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss: 2.074606418609619\n",
      "Accuracy: 0.2460000067949295\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss: 1.9805307388305664\n",
      "Accuracy: 0.2651999592781067\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss: 1.9860588312149048\n",
      "Accuracy: 0.2688000202178955\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss: 1.85579514503479\n",
      "Accuracy: 0.3133999705314636\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss: 1.7957792282104492\n",
      "Accuracy: 0.33820000290870667\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss: 1.722577452659607\n",
      "Accuracy: 0.35359999537467957\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss: 1.6562371253967285\n",
      "Accuracy: 0.3840000033378601\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss: 1.591318130493164\n",
      "Accuracy: 0.39959996938705444\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss: 1.5761971473693848\n",
      "Accuracy: 0.40420001745224\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss: 1.5708891153335571\n",
      "Accuracy: 0.41199997067451477\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss: 1.3977687358856201\n",
      "Accuracy: 0.44019997119903564\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss: 1.3065452575683594\n",
      "Accuracy: 0.4703999161720276\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss: 1.2780801057815552\n",
      "Accuracy: 0.4813999533653259\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss: 1.1774888038635254\n",
      "Accuracy: 0.48799994587898254\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss: 1.1178480386734009\n",
      "Accuracy: 0.5079999566078186\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss: 1.0251615047454834\n",
      "Accuracy: 0.5033999085426331\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss: 0.9900851845741272\n",
      "Accuracy: 0.511199951171875\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss: 1.0408077239990234\n",
      "Accuracy: 0.49539992213249207\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss: 1.0295450687408447\n",
      "Accuracy: 0.49799996614456177\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss: 0.9140365123748779\n",
      "Accuracy: 0.5241999626159668\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss: 0.7964707612991333\n",
      "Accuracy: 0.5461999177932739\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss: 0.7403419613838196\n",
      "Accuracy: 0.5509999394416809\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss: 0.6782196164131165\n",
      "Accuracy: 0.5587999224662781\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss: 0.6517696380615234\n",
      "Accuracy: 0.5483999252319336\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss: 0.6751685738563538\n",
      "Accuracy: 0.5515999794006348\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss: 0.5427950024604797\n",
      "Accuracy: 0.571199893951416\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss: 0.6029052734375\n",
      "Accuracy: 0.5407999753952026\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss: 0.5205444097518921\n",
      "Accuracy: 0.5469999313354492\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss: 0.46971482038497925\n",
      "Accuracy: 0.5607999563217163\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss: 0.5143905282020569\n",
      "Accuracy: 0.5539999008178711\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss: 0.47552579641342163\n",
      "Accuracy: 0.562999963760376\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss: 0.37720346450805664\n",
      "Accuracy: 0.5743998885154724\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss: 0.3777744770050049\n",
      "Accuracy: 0.5725999474525452\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss: 0.3554539084434509\n",
      "Accuracy: 0.5687999129295349\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss: 0.3724164366722107\n",
      "Accuracy: 0.572399914264679\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss: 0.3667524456977844\n",
      "Accuracy: 0.5349999070167542\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss: 0.3003869950771332\n",
      "Accuracy: 0.5719999074935913\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss: 0.2919221818447113\n",
      "Accuracy: 0.572999894618988\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss: 0.2721783518791199\n",
      "Accuracy: 0.5745999217033386\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss: 0.25005289912223816\n",
      "Accuracy: 0.5797999501228333\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss: 0.21429678797721863\n",
      "Accuracy: 0.5669999122619629\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss: 0.27587956190109253\n",
      "Accuracy: 0.5525999665260315\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss: 0.25380590558052063\n",
      "Accuracy: 0.5657999515533447\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss: 0.26569074392318726\n",
      "Accuracy: 0.5729999542236328\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss: 0.18359334766864777\n",
      "Accuracy: 0.5693999528884888\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss: 0.16586041450500488\n",
      "Accuracy: 0.5769999623298645\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss: 0.1492825448513031\n",
      "Accuracy: 0.5815999507904053\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss: 0.12678362429141998\n",
      "Accuracy: 0.5737999081611633\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss: 0.09884686768054962\n",
      "Accuracy: 0.5761998891830444\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss: 0.1404554694890976\n",
      "Accuracy: 0.5811999440193176\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss: 0.12504948675632477\n",
      "Accuracy: 0.5623999238014221\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss: 0.20338720083236694\n",
      "Accuracy: 0.5019999742507935\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss: 0.19327720999717712\n",
      "Accuracy: 0.55159991979599\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss: 0.14269012212753296\n",
      "Accuracy: 0.5725998878479004\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss: 0.12060869485139847\n",
      "Accuracy: 0.5799999237060547\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss: 0.10094497352838516\n",
      "Accuracy: 0.5645999312400818\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss: 0.07433810830116272\n",
      "Accuracy: 0.5937999486923218\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss: 0.07984822988510132\n",
      "Accuracy: 0.5907999277114868\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss: 0.050533492118120193\n",
      "Accuracy: 0.5983999371528625\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss: 0.04019612818956375\n",
      "Accuracy: 0.5995999574661255\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss: 0.033633992075920105\n",
      "Accuracy: 0.6061998605728149\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss: 0.04470205307006836\n",
      "Accuracy: 0.585399866104126\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss: 0.045838236808776855\n",
      "Accuracy: 0.5931999087333679\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss: 0.04960102587938309\n",
      "Accuracy: 0.5797998905181885\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss: 0.026840340346097946\n",
      "Accuracy: 0.5957998633384705\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss: 0.061495665460824966\n",
      "Accuracy: 0.5683999061584473\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss: 0.035478848963975906\n",
      "Accuracy: 0.5895999073982239\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss: 0.024974893778562546\n",
      "Accuracy: 0.5993998646736145\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss: 0.01822359673678875\n",
      "Accuracy: 0.5887998938560486\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss: 0.021136002615094185\n",
      "Accuracy: 0.5889999866485596\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss: 0.03814590349793434\n",
      "Accuracy: 0.5759998559951782\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss: 0.0256484467536211\n",
      "Accuracy: 0.5823999047279358\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss: 0.011488474905490875\n",
      "Accuracy: 0.6027998924255371\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss: 0.015311931259930134\n",
      "Accuracy: 0.5997999906539917\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss: 0.01262973714619875\n",
      "Accuracy: 0.6005998849868774\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss: 0.014949573203921318\n",
      "Accuracy: 0.5979999303817749\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss: 0.006526048295199871\n",
      "Accuracy: 0.5889999270439148\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss: 0.008827525191009045\n",
      "Accuracy: 0.6005999445915222\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss: 0.010695484466850758\n",
      "Accuracy: 0.5949999094009399\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss: 0.0083168875426054\n",
      "Accuracy: 0.5953999161720276\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss: 0.014077309519052505\n",
      "Accuracy: 0.5955999493598938\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss: 0.006249750964343548\n",
      "Accuracy: 0.5847999453544617\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss: 0.018370429053902626\n",
      "Accuracy: 0.58079993724823\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss: 0.0079274270683527\n",
      "Accuracy: 0.589199960231781\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss: 0.013660706579685211\n",
      "Accuracy: 0.5919999480247498\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss: 0.00653801579028368\n",
      "Accuracy: 0.5835999250411987\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss: 0.0074804676696658134\n",
      "Accuracy: 0.5829999446868896\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss: 0.008458743803203106\n",
      "Accuracy: 0.6001999378204346\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss: 0.011283930391073227\n",
      "Accuracy: 0.5907999277114868\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss: 0.00449763098731637\n",
      "Accuracy: 0.5853999257087708\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss: 0.00718591408804059\n",
      "Accuracy: 0.5881999731063843\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss: 0.003599521704018116\n",
      "Accuracy: 0.5979998707771301\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss: 0.0027478078845888376\n",
      "Accuracy: 0.5925998687744141\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss: 0.005043343640863895\n",
      "Accuracy: 0.5911998748779297\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss: 0.002680086763575673\n",
      "Accuracy: 0.595599889755249\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss: 0.0014143886510282755\n",
      "Accuracy: 0.5863999128341675\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss: 0.0021474449895322323\n",
      "Accuracy: 0.5877999067306519\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss: 0.002709429245442152\n",
      "Accuracy: 0.5983999371528625\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss: 0.0009624313097447157\n",
      "Accuracy: 0.598599910736084\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss: 0.001188216614536941\n",
      "Accuracy: 0.5915999412536621\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss: 0.0005025385180488229\n",
      "Accuracy: 0.6075999140739441\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss: 0.0014542937278747559\n",
      "Accuracy: 0.5957999229431152\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss: 0.0015269909054040909\n",
      "Accuracy: 0.5971999168395996\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss: 0.0015947479987517\n",
      "Accuracy: 0.5941998958587646\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss: 0.0015344471903517842\n",
      "Accuracy: 0.5949999094009399\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss: 0.0009118929738178849\n",
      "Accuracy: 0.5943999290466309\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss: 0.0010826156940311193\n",
      "Accuracy: 0.5999999046325684\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss: 0.0007553615141659975\n",
      "Accuracy: 0.5969999432563782\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss: 0.0006111062248237431\n",
      "Accuracy: 0.6063998937606812\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss: 0.0015237554907798767\n",
      "Accuracy: 0.5965999364852905\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss: 0.0022117604967206717\n",
      "Accuracy: 0.5811998844146729\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss: 0.002710537053644657\n",
      "Accuracy: 0.5951999425888062\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss: 0.0013520511565729976\n",
      "Accuracy: 0.602199912071228\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss: 0.0015810152981430292\n",
      "Accuracy: 0.6039999723434448\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss: 0.0015050783986225724\n",
      "Accuracy: 0.5981999039649963\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss: 0.0010255086235702038\n",
      "Accuracy: 0.6079999208450317\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss: 0.0004122776445001364\n",
      "Accuracy: 0.602199912071228\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss: 0.0006912741810083389\n",
      "Accuracy: 0.6053999066352844\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss: 0.00029607603210024536\n",
      "Accuracy: 0.5937999486923218\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss: 0.00348032359033823\n",
      "Accuracy: 0.598599910736084\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss: 0.0014371895231306553\n",
      "Accuracy: 0.5923998951911926\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss: 0.0008614978287369013\n",
      "Accuracy: 0.5897999405860901\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss: 0.0002831312012858689\n",
      "Accuracy: 0.5903998613357544\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss: 0.002585240639746189\n",
      "Accuracy: 0.5877999067306519\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss: 0.0005763453664258122\n",
      "Accuracy: 0.5949999094009399\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss: 0.001022916636429727\n",
      "Accuracy: 0.5959998965263367\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss: 0.0010404225904494524\n",
      "Accuracy: 0.5905999541282654\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss: 0.0012311458121985197\n",
      "Accuracy: 0.6035999059677124\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss: 0.0022319136187434196\n",
      "Accuracy: 0.5935999155044556\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss: 0.006198267452418804\n",
      "Accuracy: 0.5923999547958374\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss: 0.00186632527038455\n",
      "Accuracy: 0.5885998606681824\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss: 0.000916768389288336\n",
      "Accuracy: 0.5947999954223633\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss: 0.0008462931727990508\n",
      "Accuracy: 0.6001999378204346\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss: 0.0005158537533134222\n",
      "Accuracy: 0.5977998971939087\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss: 0.0034184278920292854\n",
      "Accuracy: 0.5927999019622803\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss: 0.002040386665612459\n",
      "Accuracy: 0.593799889087677\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss: 0.0021835980005562305\n",
      "Accuracy: 0.5895999073982239\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss: 0.0015669205458834767\n",
      "Accuracy: 0.5873998999595642\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss: 0.0011372065637260675\n",
      "Accuracy: 0.5911999344825745\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss: 0.0006708419532515109\n",
      "Accuracy: 0.5949999094009399\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss: 0.008524811826646328\n",
      "Accuracy: 0.5925998687744141\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss: 0.016833825036883354\n",
      "Accuracy: 0.5893998742103577\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss: 0.00031859235605224967\n",
      "Accuracy: 0.5983999371528625\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss: 0.0013095478061586618\n",
      "Accuracy: 0.5963999032974243\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss: 0.0005571811925619841\n",
      "Accuracy: 0.5991998910903931\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss: 0.0003593949950300157\n",
      "Accuracy: 0.6017999053001404\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss: 0.0007896758033894002\n",
      "Accuracy: 0.5965999364852905\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss: 0.00035299439332447946\n",
      "Accuracy: 0.5993999242782593\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss: 0.0002861940301954746\n",
      "Accuracy: 0.602199912071228\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss: 0.0005542280850932002\n",
      "Accuracy: 0.6005998849868774\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss: 0.0007249260670505464\n",
      "Accuracy: 0.5977999567985535\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss: 0.0004122856189496815\n",
      "Accuracy: 0.5975999236106873\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss: 0.0002323292865185067\n",
      "Accuracy: 0.6035999059677124\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss: 0.0006909670191816986\n",
      "Accuracy: 0.6003999710083008\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss: 0.0004666388558689505\n",
      "Accuracy: 0.5931999683380127\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss: 0.020104732364416122\n",
      "Accuracy: 0.5935999155044556\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss: 0.0005620449082925916\n",
      "Accuracy: 0.5893999338150024\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss: 0.005059700459241867\n",
      "Accuracy: 0.5879999399185181\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss: 0.0011680492898449302\n",
      "Accuracy: 0.5969998836517334\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss: 0.00043736500083468854\n",
      "Accuracy: 0.586199939250946\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss: 0.00020848261192440987\n",
      "Accuracy: 0.5951998829841614\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss: 0.0011563592124730349\n",
      "Accuracy: 0.6071999073028564\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss: 0.0018595753936097026\n",
      "Accuracy: 0.5999999046325684\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss: 0.002135262591764331\n",
      "Accuracy: 0.5845999717712402\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss: 0.001296093687415123\n",
      "Accuracy: 0.5993999242782593\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss: 0.005961812101304531\n",
      "Accuracy: 0.5877999663352966\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss: 0.0007705549360252917\n",
      "Accuracy: 0.6049998998641968\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss: 0.0012586060911417007\n",
      "Accuracy: 0.5967998504638672\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss: 0.0001984635746339336\n",
      "Accuracy: 0.5963999032974243\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss: 0.0008074994548223913\n",
      "Accuracy: 0.5935998558998108\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss: 0.0030044615268707275\n",
      "Accuracy: 0.5927999019622803\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss: 0.008030140772461891\n",
      "Accuracy: 0.5899999737739563\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss: 0.012362047098577023\n",
      "Accuracy: 0.5911999344825745\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss: 0.0012989871902391315\n",
      "Accuracy: 0.5953999161720276\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss: 0.0019023893401026726\n",
      "Accuracy: 0.5947999358177185\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss: 0.0034232016187161207\n",
      "Accuracy: 0.5845999121665955\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss: 0.0013939314521849155\n",
      "Accuracy: 0.5991999506950378\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss: 0.0010282968869432807\n",
      "Accuracy: 0.5977998971939087\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss: 0.002640495076775551\n",
      "Accuracy: 0.5991998910903931\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss: 0.0014473075279965997\n",
      "Accuracy: 0.6027998924255371\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss: 0.0017330076079815626\n",
      "Accuracy: 0.6055998802185059\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss: 0.0009181234054267406\n",
      "Accuracy: 0.5997998714447021\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss: 0.00014974513032939285\n",
      "Accuracy: 0.6027998924255371\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss: 0.0001368615630781278\n",
      "Accuracy: 0.6001999378204346\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss: 0.0010877334279939532\n",
      "Accuracy: 0.5921999216079712\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss: 0.02201617881655693\n",
      "Accuracy: 0.5955999493598938\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss: 0.0017257118597626686\n",
      "Accuracy: 0.5849999785423279\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss: 0.002150154672563076\n",
      "Accuracy: 0.590999960899353\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss: 0.0008422890678048134\n",
      "Accuracy: 0.5951998829841614\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss: 0.0017694238340482116\n",
      "Accuracy: 0.598599910736084\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss: 0.0015114322304725647\n",
      "Accuracy: 0.5903999209403992\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss: 0.0007094970787875354\n",
      "Accuracy: 0.6033998727798462\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss: 0.0008873819606378675\n",
      "Accuracy: 0.6005999445915222\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss: 0.0006753344787284732\n",
      "Accuracy: 0.5941998958587646\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss: 0.0007921019569039345\n",
      "Accuracy: 0.5921999216079712\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss: 0.0004684941377490759\n",
      "Accuracy: 0.6039998531341553\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss: 0.0022876414004713297\n",
      "Accuracy: 0.5883998870849609\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "print('Checking the Training on a Single Batch...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        batch_i = 1\n",
    "        for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "            train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "        print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "        print_stats(sess, batch_features, batch_labels, cost, accuracy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Fully Train the Model\n",
    "Now that you got a good accuracy with a single CIFAR-10 batch, try it with all five batches."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training...\n",
      "Epoch  1, CIFAR-10 Batch 1:  Loss: 2.2731451988220215\n",
      "Accuracy: 0.16999998688697815\n",
      "Epoch  1, CIFAR-10 Batch 2:  Loss: 2.1113789081573486\n",
      "Accuracy: 0.1905999779701233\n",
      "Epoch  1, CIFAR-10 Batch 3:  Loss: 1.9996883869171143\n",
      "Accuracy: 0.22119997441768646\n",
      "Epoch  1, CIFAR-10 Batch 4:  Loss: 1.8519176244735718\n",
      "Accuracy: 0.26659998297691345\n",
      "Epoch  1, CIFAR-10 Batch 5:  Loss: 1.861495018005371\n",
      "Accuracy: 0.30559998750686646\n",
      "Epoch  2, CIFAR-10 Batch 1:  Loss: 1.7967338562011719\n",
      "Accuracy: 0.3425999581813812\n",
      "Epoch  2, CIFAR-10 Batch 2:  Loss: 1.692315936088562\n",
      "Accuracy: 0.3636000156402588\n",
      "Epoch  2, CIFAR-10 Batch 3:  Loss: 1.449393630027771\n",
      "Accuracy: 0.4001999795436859\n",
      "Epoch  2, CIFAR-10 Batch 4:  Loss: 1.536315679550171\n",
      "Accuracy: 0.41279998421669006\n",
      "Epoch  2, CIFAR-10 Batch 5:  Loss: 1.5123865604400635\n",
      "Accuracy: 0.4131999909877777\n",
      "Epoch  3, CIFAR-10 Batch 1:  Loss: 1.5456597805023193\n",
      "Accuracy: 0.423799991607666\n",
      "Epoch  3, CIFAR-10 Batch 2:  Loss: 1.4377812147140503\n",
      "Accuracy: 0.44439998269081116\n",
      "Epoch  3, CIFAR-10 Batch 3:  Loss: 1.2498949766159058\n",
      "Accuracy: 0.47359997034072876\n",
      "Epoch  3, CIFAR-10 Batch 4:  Loss: 1.282369613647461\n",
      "Accuracy: 0.48799994587898254\n",
      "Epoch  3, CIFAR-10 Batch 5:  Loss: 1.273341417312622\n",
      "Accuracy: 0.48399993777275085\n",
      "Epoch  4, CIFAR-10 Batch 1:  Loss: 1.3090989589691162\n",
      "Accuracy: 0.5015999674797058\n",
      "Epoch  4, CIFAR-10 Batch 2:  Loss: 1.2418442964553833\n",
      "Accuracy: 0.5109999775886536\n",
      "Epoch  4, CIFAR-10 Batch 3:  Loss: 1.112949013710022\n",
      "Accuracy: 0.5225999355316162\n",
      "Epoch  4, CIFAR-10 Batch 4:  Loss: 1.1065126657485962\n",
      "Accuracy: 0.5485999584197998\n",
      "Epoch  4, CIFAR-10 Batch 5:  Loss: 1.165196418762207\n",
      "Accuracy: 0.5413999557495117\n",
      "Epoch  5, CIFAR-10 Batch 1:  Loss: 1.2090115547180176\n",
      "Accuracy: 0.5319998860359192\n",
      "Epoch  5, CIFAR-10 Batch 2:  Loss: 1.1088706254959106\n",
      "Accuracy: 0.5587999820709229\n",
      "Epoch  5, CIFAR-10 Batch 3:  Loss: 1.0123927593231201\n",
      "Accuracy: 0.5561999678611755\n",
      "Epoch  5, CIFAR-10 Batch 4:  Loss: 1.0581674575805664\n",
      "Accuracy: 0.5603999495506287\n",
      "Epoch  5, CIFAR-10 Batch 5:  Loss: 1.0336006879806519\n",
      "Accuracy: 0.5689998865127563\n",
      "Epoch  6, CIFAR-10 Batch 1:  Loss: 1.037134051322937\n",
      "Accuracy: 0.5877999067306519\n",
      "Epoch  6, CIFAR-10 Batch 2:  Loss: 1.0067838430404663\n",
      "Accuracy: 0.591999888420105\n",
      "Epoch  6, CIFAR-10 Batch 3:  Loss: 0.9662691354751587\n",
      "Accuracy: 0.5833998918533325\n",
      "Epoch  6, CIFAR-10 Batch 4:  Loss: 0.8754692673683167\n",
      "Accuracy: 0.5975999236106873\n",
      "Epoch  6, CIFAR-10 Batch 5:  Loss: 0.9727540016174316\n",
      "Accuracy: 0.5949999094009399\n",
      "Epoch  7, CIFAR-10 Batch 1:  Loss: 0.9784114956855774\n",
      "Accuracy: 0.6017999053001404\n",
      "Epoch  7, CIFAR-10 Batch 2:  Loss: 0.9291457533836365\n",
      "Accuracy: 0.5961999297142029\n",
      "Epoch  7, CIFAR-10 Batch 3:  Loss: 0.8431882262229919\n",
      "Accuracy: 0.6157999038696289\n",
      "Epoch  7, CIFAR-10 Batch 4:  Loss: 0.7711933255195618\n",
      "Accuracy: 0.6253998279571533\n",
      "Epoch  7, CIFAR-10 Batch 5:  Loss: 0.8423142433166504\n",
      "Accuracy: 0.6321998834609985\n",
      "Epoch  8, CIFAR-10 Batch 1:  Loss: 0.8639883399009705\n",
      "Accuracy: 0.6405998468399048\n",
      "Epoch  8, CIFAR-10 Batch 2:  Loss: 0.8039871454238892\n",
      "Accuracy: 0.6515998840332031\n",
      "Epoch  8, CIFAR-10 Batch 3:  Loss: 0.7596419453620911\n",
      "Accuracy: 0.6385998725891113\n",
      "Epoch  8, CIFAR-10 Batch 4:  Loss: 0.7264569997787476\n",
      "Accuracy: 0.6377999186515808\n",
      "Epoch  8, CIFAR-10 Batch 5:  Loss: 0.7463036775588989\n",
      "Accuracy: 0.6445998549461365\n",
      "Epoch  9, CIFAR-10 Batch 1:  Loss: 0.8014751672744751\n",
      "Accuracy: 0.6501998901367188\n",
      "Epoch  9, CIFAR-10 Batch 2:  Loss: 0.7254586219787598\n",
      "Accuracy: 0.6569998860359192\n",
      "Epoch  9, CIFAR-10 Batch 3:  Loss: 0.7152093052864075\n",
      "Accuracy: 0.6381999254226685\n",
      "Epoch  9, CIFAR-10 Batch 4:  Loss: 0.6769790649414062\n",
      "Accuracy: 0.6543998718261719\n",
      "Epoch  9, CIFAR-10 Batch 5:  Loss: 0.7043502330780029\n",
      "Accuracy: 0.6579999327659607\n",
      "Epoch 10, CIFAR-10 Batch 1:  Loss: 0.7197160720825195\n",
      "Accuracy: 0.6663998961448669\n",
      "Epoch 10, CIFAR-10 Batch 2:  Loss: 0.7184139490127563\n",
      "Accuracy: 0.6619999408721924\n",
      "Epoch 10, CIFAR-10 Batch 3:  Loss: 0.6098581552505493\n",
      "Accuracy: 0.6689998507499695\n",
      "Epoch 10, CIFAR-10 Batch 4:  Loss: 0.6139297485351562\n",
      "Accuracy: 0.6579998135566711\n",
      "Epoch 10, CIFAR-10 Batch 5:  Loss: 0.6339505910873413\n",
      "Accuracy: 0.6713998913764954\n",
      "Epoch 11, CIFAR-10 Batch 1:  Loss: 0.6766939163208008\n",
      "Accuracy: 0.679399847984314\n",
      "Epoch 11, CIFAR-10 Batch 2:  Loss: 0.6335484385490417\n",
      "Accuracy: 0.6813998818397522\n",
      "Epoch 11, CIFAR-10 Batch 3:  Loss: 0.532636821269989\n",
      "Accuracy: 0.676399827003479\n",
      "Epoch 11, CIFAR-10 Batch 4:  Loss: 0.5735791921615601\n",
      "Accuracy: 0.6725998520851135\n",
      "Epoch 11, CIFAR-10 Batch 5:  Loss: 0.5695381164550781\n",
      "Accuracy: 0.6857998371124268\n",
      "Epoch 12, CIFAR-10 Batch 1:  Loss: 0.5768064856529236\n",
      "Accuracy: 0.6881998777389526\n",
      "Epoch 12, CIFAR-10 Batch 2:  Loss: 0.5852105617523193\n",
      "Accuracy: 0.6845998764038086\n",
      "Epoch 12, CIFAR-10 Batch 3:  Loss: 0.5586071014404297\n",
      "Accuracy: 0.6709998846054077\n",
      "Epoch 12, CIFAR-10 Batch 4:  Loss: 0.5119412541389465\n",
      "Accuracy: 0.6861999034881592\n",
      "Epoch 12, CIFAR-10 Batch 5:  Loss: 0.49146971106529236\n",
      "Accuracy: 0.697799801826477\n",
      "Epoch 13, CIFAR-10 Batch 1:  Loss: 0.5635945796966553\n",
      "Accuracy: 0.6855998635292053\n",
      "Epoch 13, CIFAR-10 Batch 2:  Loss: 0.5531050562858582\n",
      "Accuracy: 0.6971998810768127\n",
      "Epoch 13, CIFAR-10 Batch 3:  Loss: 0.554656445980072\n",
      "Accuracy: 0.6499999165534973\n",
      "Epoch 13, CIFAR-10 Batch 4:  Loss: 0.4486980438232422\n",
      "Accuracy: 0.6705999374389648\n",
      "Epoch 13, CIFAR-10 Batch 5:  Loss: 0.45692962408065796\n",
      "Accuracy: 0.6997998356819153\n",
      "Epoch 14, CIFAR-10 Batch 1:  Loss: 0.5233824849128723\n",
      "Accuracy: 0.6923998594284058\n",
      "Epoch 14, CIFAR-10 Batch 2:  Loss: 0.5474574565887451\n",
      "Accuracy: 0.6851998567581177\n",
      "Epoch 14, CIFAR-10 Batch 3:  Loss: 0.4261404275894165\n",
      "Accuracy: 0.679399847984314\n",
      "Epoch 14, CIFAR-10 Batch 4:  Loss: 0.4312972128391266\n",
      "Accuracy: 0.6943998336791992\n",
      "Epoch 14, CIFAR-10 Batch 5:  Loss: 0.4636707007884979\n",
      "Accuracy: 0.684199869632721\n",
      "Epoch 15, CIFAR-10 Batch 1:  Loss: 0.45913517475128174\n",
      "Accuracy: 0.7095998525619507\n",
      "Epoch 15, CIFAR-10 Batch 2:  Loss: 0.4804559051990509\n",
      "Accuracy: 0.6871998906135559\n",
      "Epoch 15, CIFAR-10 Batch 3:  Loss: 0.3751443326473236\n",
      "Accuracy: 0.6985998749732971\n",
      "Epoch 15, CIFAR-10 Batch 4:  Loss: 0.3947105407714844\n",
      "Accuracy: 0.702799916267395\n",
      "Epoch 15, CIFAR-10 Batch 5:  Loss: 0.3902088403701782\n",
      "Accuracy: 0.7039998173713684\n",
      "Epoch 16, CIFAR-10 Batch 1:  Loss: 0.4334372580051422\n",
      "Accuracy: 0.6907998919487\n",
      "Epoch 16, CIFAR-10 Batch 2:  Loss: 0.4690430760383606\n",
      "Accuracy: 0.6923998594284058\n",
      "Epoch 16, CIFAR-10 Batch 3:  Loss: 0.3932197690010071\n",
      "Accuracy: 0.6975998878479004\n",
      "Epoch 16, CIFAR-10 Batch 4:  Loss: 0.375009685754776\n",
      "Accuracy: 0.7085998058319092\n",
      "Epoch 16, CIFAR-10 Batch 5:  Loss: 0.3935151696205139\n",
      "Accuracy: 0.709199845790863\n",
      "Epoch 17, CIFAR-10 Batch 1:  Loss: 0.5235000848770142\n",
      "Accuracy: 0.6751999258995056\n",
      "Epoch 17, CIFAR-10 Batch 2:  Loss: 0.435184121131897\n",
      "Accuracy: 0.6927998661994934\n",
      "Epoch 17, CIFAR-10 Batch 3:  Loss: 0.3692780137062073\n",
      "Accuracy: 0.7035998106002808\n",
      "Epoch 17, CIFAR-10 Batch 4:  Loss: 0.3411869406700134\n",
      "Accuracy: 0.7039998173713684\n",
      "Epoch 17, CIFAR-10 Batch 5:  Loss: 0.3961659073829651\n",
      "Accuracy: 0.7033998370170593\n",
      "Epoch 18, CIFAR-10 Batch 1:  Loss: 0.3739965856075287\n",
      "Accuracy: 0.7113998532295227\n",
      "Epoch 18, CIFAR-10 Batch 2:  Loss: 0.376801073551178\n",
      "Accuracy: 0.7007999420166016\n",
      "Epoch 18, CIFAR-10 Batch 3:  Loss: 0.3331529498100281\n",
      "Accuracy: 0.7043998837471008\n",
      "Epoch 18, CIFAR-10 Batch 4:  Loss: 0.29807889461517334\n",
      "Accuracy: 0.7193998694419861\n",
      "Epoch 18, CIFAR-10 Batch 5:  Loss: 0.31273147463798523\n",
      "Accuracy: 0.7119998335838318\n",
      "Epoch 19, CIFAR-10 Batch 1:  Loss: 0.30612248182296753\n",
      "Accuracy: 0.7059998512268066\n",
      "Epoch 19, CIFAR-10 Batch 2:  Loss: 0.3351593613624573\n",
      "Accuracy: 0.7055999040603638\n",
      "Epoch 19, CIFAR-10 Batch 3:  Loss: 0.3639814555644989\n",
      "Accuracy: 0.7091999053955078\n",
      "Epoch 19, CIFAR-10 Batch 4:  Loss: 0.32688236236572266\n",
      "Accuracy: 0.7033998370170593\n",
      "Epoch 19, CIFAR-10 Batch 5:  Loss: 0.29062822461128235\n",
      "Accuracy: 0.6991998553276062\n",
      "Epoch 20, CIFAR-10 Batch 1:  Loss: 0.33151906728744507\n",
      "Accuracy: 0.71399986743927\n",
      "Epoch 20, CIFAR-10 Batch 2:  Loss: 0.33405590057373047\n",
      "Accuracy: 0.7159998416900635\n",
      "Epoch 20, CIFAR-10 Batch 3:  Loss: 0.3177773952484131\n",
      "Accuracy: 0.710399866104126\n",
      "Epoch 20, CIFAR-10 Batch 4:  Loss: 0.30566349625587463\n",
      "Accuracy: 0.6995998620986938\n",
      "Epoch 20, CIFAR-10 Batch 5:  Loss: 0.2413816899061203\n",
      "Accuracy: 0.7343998551368713\n",
      "Epoch 21, CIFAR-10 Batch 1:  Loss: 0.287003755569458\n",
      "Accuracy: 0.7083998322486877\n",
      "Epoch 21, CIFAR-10 Batch 2:  Loss: 0.29976317286491394\n",
      "Accuracy: 0.7047998905181885\n",
      "Epoch 21, CIFAR-10 Batch 3:  Loss: 0.2954101264476776\n",
      "Accuracy: 0.7161998748779297\n",
      "Epoch 21, CIFAR-10 Batch 4:  Loss: 0.28353792428970337\n",
      "Accuracy: 0.685999870300293\n",
      "Epoch 21, CIFAR-10 Batch 5:  Loss: 0.3060572147369385\n",
      "Accuracy: 0.7055999040603638\n",
      "Epoch 22, CIFAR-10 Batch 1:  Loss: 0.2758941054344177\n",
      "Accuracy: 0.7145997881889343\n",
      "Epoch 22, CIFAR-10 Batch 2:  Loss: 0.2736997604370117\n",
      "Accuracy: 0.7173998355865479\n",
      "Epoch 22, CIFAR-10 Batch 3:  Loss: 0.26482293009757996\n",
      "Accuracy: 0.7283998727798462\n",
      "Epoch 22, CIFAR-10 Batch 4:  Loss: 0.24602292478084564\n",
      "Accuracy: 0.7085999250411987\n",
      "Epoch 22, CIFAR-10 Batch 5:  Loss: 0.2292272001504898\n",
      "Accuracy: 0.7111998200416565\n",
      "Epoch 23, CIFAR-10 Batch 1:  Loss: 0.2892391085624695\n",
      "Accuracy: 0.7141999006271362\n",
      "Epoch 23, CIFAR-10 Batch 2:  Loss: 0.2638546824455261\n",
      "Accuracy: 0.710399866104126\n",
      "Epoch 23, CIFAR-10 Batch 3:  Loss: 0.262346476316452\n",
      "Accuracy: 0.7081998586654663\n",
      "Epoch 23, CIFAR-10 Batch 4:  Loss: 0.20699173212051392\n",
      "Accuracy: 0.7191998362541199\n",
      "Epoch 23, CIFAR-10 Batch 5:  Loss: 0.20960351824760437\n",
      "Accuracy: 0.7143998742103577\n",
      "Epoch 24, CIFAR-10 Batch 1:  Loss: 0.22093097865581512\n",
      "Accuracy: 0.7277998328208923\n",
      "Epoch 24, CIFAR-10 Batch 2:  Loss: 0.24607622623443604\n",
      "Accuracy: 0.7047998309135437\n",
      "Epoch 24, CIFAR-10 Batch 3:  Loss: 0.25726133584976196\n",
      "Accuracy: 0.7051998376846313\n",
      "Epoch 24, CIFAR-10 Batch 4:  Loss: 0.2590208053588867\n",
      "Accuracy: 0.7015998959541321\n",
      "Epoch 24, CIFAR-10 Batch 5:  Loss: 0.20743049681186676\n",
      "Accuracy: 0.7209998369216919\n",
      "Epoch 25, CIFAR-10 Batch 1:  Loss: 0.20256216824054718\n",
      "Accuracy: 0.7173998355865479\n",
      "Epoch 25, CIFAR-10 Batch 2:  Loss: 0.2360019087791443\n",
      "Accuracy: 0.7063998579978943\n",
      "Epoch 25, CIFAR-10 Batch 3:  Loss: 0.21538619697093964\n",
      "Accuracy: 0.7077998518943787\n",
      "Epoch 25, CIFAR-10 Batch 4:  Loss: 0.20982715487480164\n",
      "Accuracy: 0.7223998308181763\n",
      "Epoch 25, CIFAR-10 Batch 5:  Loss: 0.16848711669445038\n",
      "Accuracy: 0.7293998599052429\n",
      "Epoch 26, CIFAR-10 Batch 1:  Loss: 0.16627785563468933\n",
      "Accuracy: 0.7271998524665833\n",
      "Epoch 26, CIFAR-10 Batch 2:  Loss: 0.1936645358800888\n",
      "Accuracy: 0.7205998301506042\n",
      "Epoch 26, CIFAR-10 Batch 3:  Loss: 0.25062689185142517\n",
      "Accuracy: 0.7013998627662659\n",
      "Epoch 26, CIFAR-10 Batch 4:  Loss: 0.19068185985088348\n",
      "Accuracy: 0.7107998728752136\n",
      "Epoch 26, CIFAR-10 Batch 5:  Loss: 0.18683850765228271\n",
      "Accuracy: 0.7285999059677124\n",
      "Epoch 27, CIFAR-10 Batch 1:  Loss: 0.17899158596992493\n",
      "Accuracy: 0.7203998565673828\n",
      "Epoch 27, CIFAR-10 Batch 2:  Loss: 0.19636723399162292\n",
      "Accuracy: 0.7109998464584351\n",
      "Epoch 27, CIFAR-10 Batch 3:  Loss: 0.26795530319213867\n",
      "Accuracy: 0.6949998140335083\n",
      "Epoch 27, CIFAR-10 Batch 4:  Loss: 0.21402297914028168\n",
      "Accuracy: 0.7053998708724976\n",
      "Epoch 27, CIFAR-10 Batch 5:  Loss: 0.15903687477111816\n",
      "Accuracy: 0.7319998741149902\n",
      "Epoch 28, CIFAR-10 Batch 1:  Loss: 0.17728202044963837\n",
      "Accuracy: 0.7229998111724854\n",
      "Epoch 28, CIFAR-10 Batch 2:  Loss: 0.18418914079666138\n",
      "Accuracy: 0.7183998823165894\n",
      "Epoch 28, CIFAR-10 Batch 3:  Loss: 0.20361870527267456\n",
      "Accuracy: 0.7241998910903931\n",
      "Epoch 28, CIFAR-10 Batch 4:  Loss: 0.16789548099040985\n",
      "Accuracy: 0.7165998220443726\n",
      "Epoch 28, CIFAR-10 Batch 5:  Loss: 0.15412603318691254\n",
      "Accuracy: 0.7161998748779297\n",
      "Epoch 29, CIFAR-10 Batch 1:  Loss: 0.11225900053977966\n",
      "Accuracy: 0.7399997711181641\n",
      "Epoch 29, CIFAR-10 Batch 2:  Loss: 0.17104187607765198\n",
      "Accuracy: 0.734799861907959\n",
      "Epoch 29, CIFAR-10 Batch 3:  Loss: 0.2082880139350891\n",
      "Accuracy: 0.7241998910903931\n",
      "Epoch 29, CIFAR-10 Batch 4:  Loss: 0.16956330835819244\n",
      "Accuracy: 0.7283998131752014\n",
      "Epoch 29, CIFAR-10 Batch 5:  Loss: 0.10612110048532486\n",
      "Accuracy: 0.7201998829841614\n",
      "Epoch 30, CIFAR-10 Batch 1:  Loss: 0.115610271692276\n",
      "Accuracy: 0.7379998564720154\n",
      "Epoch 30, CIFAR-10 Batch 2:  Loss: 0.14174021780490875\n",
      "Accuracy: 0.7303999066352844\n",
      "Epoch 30, CIFAR-10 Batch 3:  Loss: 0.1294727474451065\n",
      "Accuracy: 0.7381998300552368\n",
      "Epoch 30, CIFAR-10 Batch 4:  Loss: 0.16684108972549438\n",
      "Accuracy: 0.7229998111724854\n",
      "Epoch 30, CIFAR-10 Batch 5:  Loss: 0.12548919022083282\n",
      "Accuracy: 0.7259998321533203\n",
      "Epoch 31, CIFAR-10 Batch 1:  Loss: 0.11227753758430481\n",
      "Accuracy: 0.7241998314857483\n",
      "Epoch 31, CIFAR-10 Batch 2:  Loss: 0.16040939092636108\n",
      "Accuracy: 0.7279998660087585\n",
      "Epoch 31, CIFAR-10 Batch 3:  Loss: 0.12988704442977905\n",
      "Accuracy: 0.7213998436927795\n",
      "Epoch 31, CIFAR-10 Batch 4:  Loss: 0.1375875174999237\n",
      "Accuracy: 0.7177998423576355\n",
      "Epoch 31, CIFAR-10 Batch 5:  Loss: 0.08084706217050552\n",
      "Accuracy: 0.7343998551368713\n",
      "Epoch 32, CIFAR-10 Batch 1:  Loss: 0.09212885797023773\n",
      "Accuracy: 0.7281998991966248\n",
      "Epoch 32, CIFAR-10 Batch 2:  Loss: 0.1115608960390091\n",
      "Accuracy: 0.7253998517990112\n",
      "Epoch 32, CIFAR-10 Batch 3:  Loss: 0.09431999176740646\n",
      "Accuracy: 0.7397998571395874\n",
      "Epoch 32, CIFAR-10 Batch 4:  Loss: 0.13996179401874542\n",
      "Accuracy: 0.7169998288154602\n",
      "Epoch 32, CIFAR-10 Batch 5:  Loss: 0.09089849889278412\n",
      "Accuracy: 0.7269998788833618\n",
      "Epoch 33, CIFAR-10 Batch 1:  Loss: 0.1335529237985611\n",
      "Accuracy: 0.7341998219490051\n",
      "Epoch 33, CIFAR-10 Batch 2:  Loss: 0.1240830048918724\n",
      "Accuracy: 0.7235998511314392\n",
      "Epoch 33, CIFAR-10 Batch 3:  Loss: 0.0965564101934433\n",
      "Accuracy: 0.72819983959198\n",
      "Epoch 33, CIFAR-10 Batch 4:  Loss: 0.11312513053417206\n",
      "Accuracy: 0.7143998742103577\n",
      "Epoch 33, CIFAR-10 Batch 5:  Loss: 0.13539902865886688\n",
      "Accuracy: 0.7035998106002808\n",
      "Epoch 34, CIFAR-10 Batch 1:  Loss: 0.10434944927692413\n",
      "Accuracy: 0.7203998565673828\n",
      "Epoch 34, CIFAR-10 Batch 2:  Loss: 0.10813908278942108\n",
      "Accuracy: 0.7261998653411865\n",
      "Epoch 34, CIFAR-10 Batch 3:  Loss: 0.1021711677312851\n",
      "Accuracy: 0.7337998151779175\n",
      "Epoch 34, CIFAR-10 Batch 4:  Loss: 0.09637534618377686\n",
      "Accuracy: 0.7109998464584351\n",
      "Epoch 34, CIFAR-10 Batch 5:  Loss: 0.0649803876876831\n",
      "Accuracy: 0.729999840259552\n",
      "Epoch 35, CIFAR-10 Batch 1:  Loss: 0.09623493254184723\n",
      "Accuracy: 0.727199912071228\n",
      "Epoch 35, CIFAR-10 Batch 2:  Loss: 0.08942979574203491\n",
      "Accuracy: 0.732999861240387\n",
      "Epoch 35, CIFAR-10 Batch 3:  Loss: 0.11964232474565506\n",
      "Accuracy: 0.7339999079704285\n",
      "Epoch 35, CIFAR-10 Batch 4:  Loss: 0.12462033331394196\n",
      "Accuracy: 0.7189998626708984\n",
      "Epoch 35, CIFAR-10 Batch 5:  Loss: 0.06823275983333588\n",
      "Accuracy: 0.7435998320579529\n",
      "Epoch 36, CIFAR-10 Batch 1:  Loss: 0.08019429445266724\n",
      "Accuracy: 0.7337998747825623\n",
      "Epoch 36, CIFAR-10 Batch 2:  Loss: 0.09426634758710861\n",
      "Accuracy: 0.72819983959198\n",
      "Epoch 36, CIFAR-10 Batch 3:  Loss: 0.08205967396497726\n",
      "Accuracy: 0.7375998497009277\n",
      "Epoch 36, CIFAR-10 Batch 4:  Loss: 0.08835127204656601\n",
      "Accuracy: 0.7391998171806335\n",
      "Epoch 36, CIFAR-10 Batch 5:  Loss: 0.06282052397727966\n",
      "Accuracy: 0.7393998503684998\n",
      "Epoch 37, CIFAR-10 Batch 1:  Loss: 0.10155414044857025\n",
      "Accuracy: 0.7275997996330261\n",
      "Epoch 37, CIFAR-10 Batch 2:  Loss: 0.08879142999649048\n",
      "Accuracy: 0.7257998585700989\n",
      "Epoch 37, CIFAR-10 Batch 3:  Loss: 0.09539855271577835\n",
      "Accuracy: 0.7253998517990112\n",
      "Epoch 37, CIFAR-10 Batch 4:  Loss: 0.07661315053701401\n",
      "Accuracy: 0.7277998924255371\n",
      "Epoch 37, CIFAR-10 Batch 5:  Loss: 0.06941353529691696\n",
      "Accuracy: 0.7313998341560364\n",
      "Epoch 38, CIFAR-10 Batch 1:  Loss: 0.0481262244284153\n",
      "Accuracy: 0.745999813079834\n",
      "Epoch 38, CIFAR-10 Batch 2:  Loss: 0.08960544317960739\n",
      "Accuracy: 0.710399866104126\n",
      "Epoch 38, CIFAR-10 Batch 3:  Loss: 0.06784003973007202\n",
      "Accuracy: 0.7269998788833618\n",
      "Epoch 38, CIFAR-10 Batch 4:  Loss: 0.07764678448438644\n",
      "Accuracy: 0.721599817276001\n",
      "Epoch 38, CIFAR-10 Batch 5:  Loss: 0.05702252313494682\n",
      "Accuracy: 0.7267998456954956\n",
      "Epoch 39, CIFAR-10 Batch 1:  Loss: 0.05680813640356064\n",
      "Accuracy: 0.7381998300552368\n",
      "Epoch 39, CIFAR-10 Batch 2:  Loss: 0.055464968085289\n",
      "Accuracy: 0.7355998754501343\n",
      "Epoch 39, CIFAR-10 Batch 3:  Loss: 0.06553097069263458\n",
      "Accuracy: 0.7321998476982117\n",
      "Epoch 39, CIFAR-10 Batch 4:  Loss: 0.07164560258388519\n",
      "Accuracy: 0.7287998795509338\n",
      "Epoch 39, CIFAR-10 Batch 5:  Loss: 0.06878063827753067\n",
      "Accuracy: 0.7319998741149902\n",
      "Epoch 40, CIFAR-10 Batch 1:  Loss: 0.06886318325996399\n",
      "Accuracy: 0.7241998910903931\n",
      "Epoch 40, CIFAR-10 Batch 2:  Loss: 0.06881953775882721\n",
      "Accuracy: 0.7263997793197632\n",
      "Epoch 40, CIFAR-10 Batch 3:  Loss: 0.07412850111722946\n",
      "Accuracy: 0.7201998829841614\n",
      "Epoch 40, CIFAR-10 Batch 4:  Loss: 0.058332569897174835\n",
      "Accuracy: 0.731599748134613\n",
      "Epoch 40, CIFAR-10 Batch 5:  Loss: 0.0540216788649559\n",
      "Accuracy: 0.7343998551368713\n",
      "Epoch 41, CIFAR-10 Batch 1:  Loss: 0.08063353598117828\n",
      "Accuracy: 0.7097998261451721\n",
      "Epoch 41, CIFAR-10 Batch 2:  Loss: 0.07322913408279419\n",
      "Accuracy: 0.7187998294830322\n",
      "Epoch 41, CIFAR-10 Batch 3:  Loss: 0.06902682781219482\n",
      "Accuracy: 0.7225998044013977\n",
      "Epoch 41, CIFAR-10 Batch 4:  Loss: 0.07856453955173492\n",
      "Accuracy: 0.729999840259552\n",
      "Epoch 41, CIFAR-10 Batch 5:  Loss: 0.054989442229270935\n",
      "Accuracy: 0.7265998125076294\n",
      "Epoch 42, CIFAR-10 Batch 1:  Loss: 0.04989277198910713\n",
      "Accuracy: 0.7287998199462891\n",
      "Epoch 42, CIFAR-10 Batch 2:  Loss: 0.059524573385715485\n",
      "Accuracy: 0.7211998701095581\n",
      "Epoch 42, CIFAR-10 Batch 3:  Loss: 0.08127317577600479\n",
      "Accuracy: 0.7191998362541199\n",
      "Epoch 42, CIFAR-10 Batch 4:  Loss: 0.07447689771652222\n",
      "Accuracy: 0.716999888420105\n",
      "Epoch 42, CIFAR-10 Batch 5:  Loss: 0.09133569151163101\n",
      "Accuracy: 0.7177999019622803\n",
      "Epoch 43, CIFAR-10 Batch 1:  Loss: 0.06448516994714737\n",
      "Accuracy: 0.7165998220443726\n",
      "Epoch 43, CIFAR-10 Batch 2:  Loss: 0.07496306300163269\n",
      "Accuracy: 0.722399890422821\n",
      "Epoch 43, CIFAR-10 Batch 3:  Loss: 0.09839697182178497\n",
      "Accuracy: 0.715799868106842\n",
      "Epoch 43, CIFAR-10 Batch 4:  Loss: 0.07167456299066544\n",
      "Accuracy: 0.7293998599052429\n",
      "Epoch 43, CIFAR-10 Batch 5:  Loss: 0.0423785038292408\n",
      "Accuracy: 0.7331997752189636\n",
      "Epoch 44, CIFAR-10 Batch 1:  Loss: 0.0498032309114933\n",
      "Accuracy: 0.7343997955322266\n",
      "Epoch 44, CIFAR-10 Batch 2:  Loss: 0.03801286220550537\n",
      "Accuracy: 0.7367998361587524\n",
      "Epoch 44, CIFAR-10 Batch 3:  Loss: 0.05471568554639816\n",
      "Accuracy: 0.7229998707771301\n",
      "Epoch 44, CIFAR-10 Batch 4:  Loss: 0.04587836563587189\n",
      "Accuracy: 0.7295998930931091\n",
      "Epoch 44, CIFAR-10 Batch 5:  Loss: 0.06436540186405182\n",
      "Accuracy: 0.7289998531341553\n",
      "Epoch 45, CIFAR-10 Batch 1:  Loss: 0.0498947948217392\n",
      "Accuracy: 0.7369998693466187\n",
      "Epoch 45, CIFAR-10 Batch 2:  Loss: 0.03669048473238945\n",
      "Accuracy: 0.7399998307228088\n",
      "Epoch 45, CIFAR-10 Batch 3:  Loss: 0.04069316014647484\n",
      "Accuracy: 0.733599841594696\n",
      "Epoch 45, CIFAR-10 Batch 4:  Loss: 0.07114067673683167\n",
      "Accuracy: 0.7311998605728149\n",
      "Epoch 45, CIFAR-10 Batch 5:  Loss: 0.03520854935050011\n",
      "Accuracy: 0.739599883556366\n",
      "Epoch 46, CIFAR-10 Batch 1:  Loss: 0.04296649992465973\n",
      "Accuracy: 0.7373998165130615\n",
      "Epoch 46, CIFAR-10 Batch 2:  Loss: 0.050878819078207016\n",
      "Accuracy: 0.7295998334884644\n",
      "Epoch 46, CIFAR-10 Batch 3:  Loss: 0.040557507425546646\n",
      "Accuracy: 0.7469998002052307\n",
      "Epoch 46, CIFAR-10 Batch 4:  Loss: 0.06335862725973129\n",
      "Accuracy: 0.7405998110771179\n",
      "Epoch 46, CIFAR-10 Batch 5:  Loss: 0.025865767151117325\n",
      "Accuracy: 0.7375998497009277\n",
      "Epoch 47, CIFAR-10 Batch 1:  Loss: 0.03233804181218147\n",
      "Accuracy: 0.7331998348236084\n",
      "Epoch 47, CIFAR-10 Batch 2:  Loss: 0.02953179180622101\n",
      "Accuracy: 0.7389999032020569\n",
      "Epoch 47, CIFAR-10 Batch 3:  Loss: 0.03962799534201622\n",
      "Accuracy: 0.7325998544692993\n",
      "Epoch 47, CIFAR-10 Batch 4:  Loss: 0.0540219321846962\n",
      "Accuracy: 0.716999888420105\n",
      "Epoch 47, CIFAR-10 Batch 5:  Loss: 0.024587301537394524\n",
      "Accuracy: 0.7379997968673706\n",
      "Epoch 48, CIFAR-10 Batch 1:  Loss: 0.047995660454034805\n",
      "Accuracy: 0.7345999479293823\n",
      "Epoch 48, CIFAR-10 Batch 2:  Loss: 0.03915639966726303\n",
      "Accuracy: 0.7433998584747314\n",
      "Epoch 48, CIFAR-10 Batch 3:  Loss: 0.04050526022911072\n",
      "Accuracy: 0.7347998023033142\n",
      "Epoch 48, CIFAR-10 Batch 4:  Loss: 0.042194344103336334\n",
      "Accuracy: 0.7295998334884644\n",
      "Epoch 48, CIFAR-10 Batch 5:  Loss: 0.024234352633357048\n",
      "Accuracy: 0.7397998571395874\n",
      "Epoch 49, CIFAR-10 Batch 1:  Loss: 0.043637361377477646\n",
      "Accuracy: 0.7359998822212219\n",
      "Epoch 49, CIFAR-10 Batch 2:  Loss: 0.025383437052369118\n",
      "Accuracy: 0.7487998008728027\n",
      "Epoch 49, CIFAR-10 Batch 3:  Loss: 0.02941269427537918\n",
      "Accuracy: 0.7431998252868652\n",
      "Epoch 49, CIFAR-10 Batch 4:  Loss: 0.04522128775715828\n",
      "Accuracy: 0.731799840927124\n",
      "Epoch 49, CIFAR-10 Batch 5:  Loss: 0.02407926693558693\n",
      "Accuracy: 0.7355998158454895\n",
      "Epoch 50, CIFAR-10 Batch 1:  Loss: 0.030597703531384468\n",
      "Accuracy: 0.7377998232841492\n",
      "Epoch 50, CIFAR-10 Batch 2:  Loss: 0.021424151957035065\n",
      "Accuracy: 0.7421998381614685\n",
      "Epoch 50, CIFAR-10 Batch 3:  Loss: 0.02852155826985836\n",
      "Accuracy: 0.7357999086380005\n",
      "Epoch 50, CIFAR-10 Batch 4:  Loss: 0.023196417838335037\n",
      "Accuracy: 0.7359998226165771\n",
      "Epoch 50, CIFAR-10 Batch 5:  Loss: 0.020593877881765366\n",
      "Accuracy: 0.7373998761177063\n",
      "Epoch 51, CIFAR-10 Batch 1:  Loss: 0.018589409068226814\n",
      "Accuracy: 0.7443997859954834\n",
      "Epoch 51, CIFAR-10 Batch 2:  Loss: 0.02314407378435135\n",
      "Accuracy: 0.7425999045372009\n",
      "Epoch 51, CIFAR-10 Batch 3:  Loss: 0.02455015480518341\n",
      "Accuracy: 0.7425997853279114\n",
      "Epoch 51, CIFAR-10 Batch 4:  Loss: 0.0586080327630043\n",
      "Accuracy: 0.7279998660087585\n",
      "Epoch 51, CIFAR-10 Batch 5:  Loss: 0.021376382559537888\n",
      "Accuracy: 0.7281997799873352\n",
      "Epoch 52, CIFAR-10 Batch 1:  Loss: 0.025711337104439735\n",
      "Accuracy: 0.7359998822212219\n",
      "Epoch 52, CIFAR-10 Batch 2:  Loss: 0.01792987994849682\n",
      "Accuracy: 0.7393998503684998\n",
      "Epoch 52, CIFAR-10 Batch 3:  Loss: 0.03834262490272522\n",
      "Accuracy: 0.73639976978302\n",
      "Epoch 52, CIFAR-10 Batch 4:  Loss: 0.022233620285987854\n",
      "Accuracy: 0.7391998171806335\n",
      "Epoch 52, CIFAR-10 Batch 5:  Loss: 0.030232295393943787\n",
      "Accuracy: 0.7353998422622681\n",
      "Epoch 53, CIFAR-10 Batch 1:  Loss: 0.019224796444177628\n",
      "Accuracy: 0.7259998917579651\n",
      "Epoch 53, CIFAR-10 Batch 2:  Loss: 0.02640916220843792\n",
      "Accuracy: 0.7355998158454895\n",
      "Epoch 53, CIFAR-10 Batch 3:  Loss: 0.019027259200811386\n",
      "Accuracy: 0.7503998279571533\n",
      "Epoch 53, CIFAR-10 Batch 4:  Loss: 0.02024228125810623\n",
      "Accuracy: 0.7419998645782471\n",
      "Epoch 53, CIFAR-10 Batch 5:  Loss: 0.011237502098083496\n",
      "Accuracy: 0.7467998266220093\n",
      "Epoch 54, CIFAR-10 Batch 1:  Loss: 0.04142030328512192\n",
      "Accuracy: 0.7415998578071594\n",
      "Epoch 54, CIFAR-10 Batch 2:  Loss: 0.011871288530528545\n",
      "Accuracy: 0.741399884223938\n",
      "Epoch 54, CIFAR-10 Batch 3:  Loss: 0.019944822415709496\n",
      "Accuracy: 0.7383998036384583\n",
      "Epoch 54, CIFAR-10 Batch 4:  Loss: 0.02074526622891426\n",
      "Accuracy: 0.7475998401641846\n",
      "Epoch 54, CIFAR-10 Batch 5:  Loss: 0.02142518386244774\n",
      "Accuracy: 0.744999885559082\n",
      "Epoch 55, CIFAR-10 Batch 1:  Loss: 0.01099675428122282\n",
      "Accuracy: 0.739399790763855\n",
      "Epoch 55, CIFAR-10 Batch 2:  Loss: 0.007230905815958977\n",
      "Accuracy: 0.7449998259544373\n",
      "Epoch 55, CIFAR-10 Batch 3:  Loss: 0.013749511912465096\n",
      "Accuracy: 0.7437998652458191\n",
      "Epoch 55, CIFAR-10 Batch 4:  Loss: 0.015459772199392319\n",
      "Accuracy: 0.7391998767852783\n",
      "Epoch 55, CIFAR-10 Batch 5:  Loss: 0.012685667723417282\n",
      "Accuracy: 0.7463998198509216\n",
      "Epoch 56, CIFAR-10 Batch 1:  Loss: 0.026458822190761566\n",
      "Accuracy: 0.7395998239517212\n",
      "Epoch 56, CIFAR-10 Batch 2:  Loss: 0.014550854451954365\n",
      "Accuracy: 0.7373998165130615\n",
      "Epoch 56, CIFAR-10 Batch 3:  Loss: 0.012025442905724049\n",
      "Accuracy: 0.7515997886657715\n",
      "Epoch 56, CIFAR-10 Batch 4:  Loss: 0.02057838626205921\n",
      "Accuracy: 0.7345998883247375\n",
      "Epoch 56, CIFAR-10 Batch 5:  Loss: 0.01319803111255169\n",
      "Accuracy: 0.75139981508255\n",
      "Epoch 57, CIFAR-10 Batch 1:  Loss: 0.02009427174925804\n",
      "Accuracy: 0.7407999038696289\n",
      "Epoch 57, CIFAR-10 Batch 2:  Loss: 0.015680596232414246\n",
      "Accuracy: 0.7443998456001282\n",
      "Epoch 57, CIFAR-10 Batch 3:  Loss: 0.022771287709474564\n",
      "Accuracy: 0.7373999357223511\n",
      "Epoch 57, CIFAR-10 Batch 4:  Loss: 0.016289785504341125\n",
      "Accuracy: 0.7453998327255249\n",
      "Epoch 57, CIFAR-10 Batch 5:  Loss: 0.008458349853754044\n",
      "Accuracy: 0.7343998551368713\n",
      "Epoch 58, CIFAR-10 Batch 1:  Loss: 0.011512738652527332\n",
      "Accuracy: 0.7425998449325562\n",
      "Epoch 58, CIFAR-10 Batch 2:  Loss: 0.014644160866737366\n",
      "Accuracy: 0.7409998774528503\n",
      "Epoch 58, CIFAR-10 Batch 3:  Loss: 0.016762087121605873\n",
      "Accuracy: 0.7301998734474182\n",
      "Epoch 58, CIFAR-10 Batch 4:  Loss: 0.023695724084973335\n",
      "Accuracy: 0.74319988489151\n",
      "Epoch 58, CIFAR-10 Batch 5:  Loss: 0.010810550302267075\n",
      "Accuracy: 0.7437998056411743\n",
      "Epoch 59, CIFAR-10 Batch 1:  Loss: 0.016756422817707062\n",
      "Accuracy: 0.7317999005317688\n",
      "Epoch 59, CIFAR-10 Batch 2:  Loss: 0.015719598159193993\n",
      "Accuracy: 0.739399790763855\n",
      "Epoch 59, CIFAR-10 Batch 3:  Loss: 0.022290151566267014\n",
      "Accuracy: 0.740199863910675\n",
      "Epoch 59, CIFAR-10 Batch 4:  Loss: 0.017207294702529907\n",
      "Accuracy: 0.731799840927124\n",
      "Epoch 59, CIFAR-10 Batch 5:  Loss: 0.022797903046011925\n",
      "Accuracy: 0.7335997819900513\n",
      "Epoch 60, CIFAR-10 Batch 1:  Loss: 0.03658640384674072\n",
      "Accuracy: 0.7261998653411865\n",
      "Epoch 60, CIFAR-10 Batch 2:  Loss: 0.016521863639354706\n",
      "Accuracy: 0.7335997819900513\n",
      "Epoch 60, CIFAR-10 Batch 3:  Loss: 0.02478320151567459\n",
      "Accuracy: 0.733599841594696\n",
      "Epoch 60, CIFAR-10 Batch 4:  Loss: 0.015307951718568802\n",
      "Accuracy: 0.7387997508049011\n",
      "Epoch 60, CIFAR-10 Batch 5:  Loss: 0.008776159957051277\n",
      "Accuracy: 0.7407999038696289\n",
      "Epoch 61, CIFAR-10 Batch 1:  Loss: 0.007145179435610771\n",
      "Accuracy: 0.7391998767852783\n",
      "Epoch 61, CIFAR-10 Batch 2:  Loss: 0.012192811816930771\n",
      "Accuracy: 0.7323998212814331\n",
      "Epoch 61, CIFAR-10 Batch 3:  Loss: 0.012892448343336582\n",
      "Accuracy: 0.7415997982025146\n",
      "Epoch 61, CIFAR-10 Batch 4:  Loss: 0.011463255621492863\n",
      "Accuracy: 0.7439998388290405\n",
      "Epoch 61, CIFAR-10 Batch 5:  Loss: 0.009049324318766594\n",
      "Accuracy: 0.7401999235153198\n",
      "Epoch 62, CIFAR-10 Batch 1:  Loss: 0.020777512341737747\n",
      "Accuracy: 0.7293998003005981\n",
      "Epoch 62, CIFAR-10 Batch 2:  Loss: 0.03192243352532387\n",
      "Accuracy: 0.7397998571395874\n",
      "Epoch 62, CIFAR-10 Batch 3:  Loss: 0.017091883346438408\n",
      "Accuracy: 0.7293998599052429\n",
      "Epoch 62, CIFAR-10 Batch 4:  Loss: 0.02524237334728241\n",
      "Accuracy: 0.7409998774528503\n",
      "Epoch 62, CIFAR-10 Batch 5:  Loss: 0.010171744041144848\n",
      "Accuracy: 0.7391998767852783\n",
      "Epoch 63, CIFAR-10 Batch 1:  Loss: 0.014919424429535866\n",
      "Accuracy: 0.7357998490333557\n",
      "Epoch 63, CIFAR-10 Batch 2:  Loss: 0.019685350358486176\n",
      "Accuracy: 0.7329999208450317\n",
      "Epoch 63, CIFAR-10 Batch 3:  Loss: 0.01022196002304554\n",
      "Accuracy: 0.7367998361587524\n",
      "Epoch 63, CIFAR-10 Batch 4:  Loss: 0.013601898215711117\n",
      "Accuracy: 0.7411998510360718\n",
      "Epoch 63, CIFAR-10 Batch 5:  Loss: 0.01622297428548336\n",
      "Accuracy: 0.7289998531341553\n",
      "Epoch 64, CIFAR-10 Batch 1:  Loss: 0.037455059587955475\n",
      "Accuracy: 0.7205998301506042\n",
      "Epoch 64, CIFAR-10 Batch 2:  Loss: 0.013874088414013386\n",
      "Accuracy: 0.7343998551368713\n",
      "Epoch 64, CIFAR-10 Batch 3:  Loss: 0.012076258659362793\n",
      "Accuracy: 0.7387998104095459\n",
      "Epoch 64, CIFAR-10 Batch 4:  Loss: 0.015195850282907486\n",
      "Accuracy: 0.7377998232841492\n",
      "Epoch 64, CIFAR-10 Batch 5:  Loss: 0.008471759967505932\n",
      "Accuracy: 0.7369998693466187\n",
      "Epoch 65, CIFAR-10 Batch 1:  Loss: 0.01984025165438652\n",
      "Accuracy: 0.732999861240387\n",
      "Epoch 65, CIFAR-10 Batch 2:  Loss: 0.02574186399579048\n",
      "Accuracy: 0.7261998653411865\n",
      "Epoch 65, CIFAR-10 Batch 3:  Loss: 0.011197209358215332\n",
      "Accuracy: 0.734799861907959\n",
      "Epoch 65, CIFAR-10 Batch 4:  Loss: 0.007065669167786837\n",
      "Accuracy: 0.7483998537063599\n",
      "Epoch 65, CIFAR-10 Batch 5:  Loss: 0.006385107059031725\n",
      "Accuracy: 0.7457998394966125\n",
      "Epoch 66, CIFAR-10 Batch 1:  Loss: 0.009554381482303143\n",
      "Accuracy: 0.7431998252868652\n",
      "Epoch 66, CIFAR-10 Batch 2:  Loss: 0.020176228135824203\n",
      "Accuracy: 0.7343998551368713\n",
      "Epoch 66, CIFAR-10 Batch 3:  Loss: 0.015966283157467842\n",
      "Accuracy: 0.7381998896598816\n",
      "Epoch 66, CIFAR-10 Batch 4:  Loss: 0.013877728953957558\n",
      "Accuracy: 0.7487998604774475\n",
      "Epoch 66, CIFAR-10 Batch 5:  Loss: 0.010532034561038017\n",
      "Accuracy: 0.7503998279571533\n",
      "Epoch 67, CIFAR-10 Batch 1:  Loss: 0.029764221981167793\n",
      "Accuracy: 0.7351998090744019\n",
      "Epoch 67, CIFAR-10 Batch 2:  Loss: 0.02452446147799492\n",
      "Accuracy: 0.7321999073028564\n",
      "Epoch 67, CIFAR-10 Batch 3:  Loss: 0.007045401725918055\n",
      "Accuracy: 0.744399905204773\n",
      "Epoch 67, CIFAR-10 Batch 4:  Loss: 0.01296389102935791\n",
      "Accuracy: 0.7363998889923096\n",
      "Epoch 67, CIFAR-10 Batch 5:  Loss: 0.03087087906897068\n",
      "Accuracy: 0.7315999269485474\n",
      "Epoch 68, CIFAR-10 Batch 1:  Loss: 0.02207537367939949\n",
      "Accuracy: 0.7319998741149902\n",
      "Epoch 68, CIFAR-10 Batch 2:  Loss: 0.032521430402994156\n",
      "Accuracy: 0.7405998706817627\n",
      "Epoch 68, CIFAR-10 Batch 3:  Loss: 0.011379094794392586\n",
      "Accuracy: 0.734799861907959\n",
      "Epoch 68, CIFAR-10 Batch 4:  Loss: 0.01460186205804348\n",
      "Accuracy: 0.7403998374938965\n",
      "Epoch 68, CIFAR-10 Batch 5:  Loss: 0.004721758421510458\n",
      "Accuracy: 0.7365998029708862\n",
      "Epoch 69, CIFAR-10 Batch 1:  Loss: 0.006957939825952053\n",
      "Accuracy: 0.7415997982025146\n",
      "Epoch 69, CIFAR-10 Batch 2:  Loss: 0.01349976658821106\n",
      "Accuracy: 0.7241998910903931\n",
      "Epoch 69, CIFAR-10 Batch 3:  Loss: 0.01881769485771656\n",
      "Accuracy: 0.7299998998641968\n",
      "Epoch 69, CIFAR-10 Batch 4:  Loss: 0.012186191976070404\n",
      "Accuracy: 0.7457998991012573\n",
      "Epoch 69, CIFAR-10 Batch 5:  Loss: 0.0059486376121640205\n",
      "Accuracy: 0.7411998510360718\n",
      "Epoch 70, CIFAR-10 Batch 1:  Loss: 0.013707899488508701\n",
      "Accuracy: 0.7379998564720154\n",
      "Epoch 70, CIFAR-10 Batch 2:  Loss: 0.008761529810726643\n",
      "Accuracy: 0.7343998551368713\n",
      "Epoch 70, CIFAR-10 Batch 3:  Loss: 0.014173556119203568\n",
      "Accuracy: 0.7311998009681702\n",
      "Epoch 70, CIFAR-10 Batch 4:  Loss: 0.012669732794165611\n",
      "Accuracy: 0.7351998686790466\n",
      "Epoch 70, CIFAR-10 Batch 5:  Loss: 0.006440837401896715\n",
      "Accuracy: 0.7295998334884644\n",
      "Epoch 71, CIFAR-10 Batch 1:  Loss: 0.01036728173494339\n",
      "Accuracy: 0.7229998111724854\n",
      "Epoch 71, CIFAR-10 Batch 2:  Loss: 0.006550823803991079\n",
      "Accuracy: 0.7265998721122742\n",
      "Epoch 71, CIFAR-10 Batch 3:  Loss: 0.006462148390710354\n",
      "Accuracy: 0.7379997968673706\n",
      "Epoch 71, CIFAR-10 Batch 4:  Loss: 0.00886572990566492\n",
      "Accuracy: 0.7357998490333557\n",
      "Epoch 71, CIFAR-10 Batch 5:  Loss: 0.008207974024116993\n",
      "Accuracy: 0.7343998551368713\n",
      "Epoch 72, CIFAR-10 Batch 1:  Loss: 0.018851809203624725\n",
      "Accuracy: 0.7407999038696289\n",
      "Epoch 72, CIFAR-10 Batch 2:  Loss: 0.004110714420676231\n",
      "Accuracy: 0.7363998293876648\n",
      "Epoch 72, CIFAR-10 Batch 3:  Loss: 0.008705539628863335\n",
      "Accuracy: 0.731799840927124\n",
      "Epoch 72, CIFAR-10 Batch 4:  Loss: 0.005284581799060106\n",
      "Accuracy: 0.7413998246192932\n",
      "Epoch 72, CIFAR-10 Batch 5:  Loss: 0.00742223160341382\n",
      "Accuracy: 0.7419998645782471\n",
      "Epoch 73, CIFAR-10 Batch 1:  Loss: 0.006429238710552454\n",
      "Accuracy: 0.7337998151779175\n",
      "Epoch 73, CIFAR-10 Batch 2:  Loss: 0.013434912078082561\n",
      "Accuracy: 0.7339998483657837\n",
      "Epoch 73, CIFAR-10 Batch 3:  Loss: 0.004694018978625536\n",
      "Accuracy: 0.7341998815536499\n",
      "Epoch 73, CIFAR-10 Batch 4:  Loss: 0.011558246798813343\n",
      "Accuracy: 0.7369998097419739\n",
      "Epoch 73, CIFAR-10 Batch 5:  Loss: 0.009027821943163872\n",
      "Accuracy: 0.7345998883247375\n",
      "Epoch 74, CIFAR-10 Batch 1:  Loss: 0.007982034236192703\n",
      "Accuracy: 0.7383998036384583\n",
      "Epoch 74, CIFAR-10 Batch 2:  Loss: 0.013693736866116524\n",
      "Accuracy: 0.7359998822212219\n",
      "Epoch 74, CIFAR-10 Batch 3:  Loss: 0.013339865952730179\n",
      "Accuracy: 0.7293998599052429\n",
      "Epoch 74, CIFAR-10 Batch 4:  Loss: 0.01463509164750576\n",
      "Accuracy: 0.7391998171806335\n",
      "Epoch 74, CIFAR-10 Batch 5:  Loss: 0.00541093060746789\n",
      "Accuracy: 0.7459998726844788\n",
      "Epoch 75, CIFAR-10 Batch 1:  Loss: 0.009869806468486786\n",
      "Accuracy: 0.7425998449325562\n",
      "Epoch 75, CIFAR-10 Batch 2:  Loss: 0.0032858699560165405\n",
      "Accuracy: 0.7383998036384583\n",
      "Epoch 75, CIFAR-10 Batch 3:  Loss: 0.016723453998565674\n",
      "Accuracy: 0.7341998815536499\n",
      "Epoch 75, CIFAR-10 Batch 4:  Loss: 0.014841428957879543\n",
      "Accuracy: 0.7331998944282532\n",
      "Epoch 75, CIFAR-10 Batch 5:  Loss: 0.00863654911518097\n",
      "Accuracy: 0.7385998368263245\n",
      "Epoch 76, CIFAR-10 Batch 1:  Loss: 0.008610706776380539\n",
      "Accuracy: 0.7367997765541077\n",
      "Epoch 76, CIFAR-10 Batch 2:  Loss: 0.008437586016952991\n",
      "Accuracy: 0.7429999113082886\n",
      "Epoch 76, CIFAR-10 Batch 3:  Loss: 0.007164094131439924\n",
      "Accuracy: 0.7373998165130615\n",
      "Epoch 76, CIFAR-10 Batch 4:  Loss: 0.008979045785963535\n",
      "Accuracy: 0.7473998665809631\n",
      "Epoch 76, CIFAR-10 Batch 5:  Loss: 0.015762394294142723\n",
      "Accuracy: 0.737599790096283\n",
      "Epoch 77, CIFAR-10 Batch 1:  Loss: 0.007416984066367149\n",
      "Accuracy: 0.7343999147415161\n",
      "Epoch 77, CIFAR-10 Batch 2:  Loss: 0.0068715037778019905\n",
      "Accuracy: 0.7307998538017273\n",
      "Epoch 77, CIFAR-10 Batch 3:  Loss: 0.00671972893178463\n",
      "Accuracy: 0.7421998381614685\n",
      "Epoch 77, CIFAR-10 Batch 4:  Loss: 0.010280810296535492\n",
      "Accuracy: 0.7417997717857361\n",
      "Epoch 77, CIFAR-10 Batch 5:  Loss: 0.010059135034680367\n",
      "Accuracy: 0.7511998414993286\n",
      "Epoch 78, CIFAR-10 Batch 1:  Loss: 0.0040150899440050125\n",
      "Accuracy: 0.7473998069763184\n",
      "Epoch 78, CIFAR-10 Batch 2:  Loss: 0.00225265184417367\n",
      "Accuracy: 0.7423998117446899\n",
      "Epoch 78, CIFAR-10 Batch 3:  Loss: 0.010540538467466831\n",
      "Accuracy: 0.7331998348236084\n",
      "Epoch 78, CIFAR-10 Batch 4:  Loss: 0.0027610042598098516\n",
      "Accuracy: 0.7483997941017151\n",
      "Epoch 78, CIFAR-10 Batch 5:  Loss: 0.008622262626886368\n",
      "Accuracy: 0.7405998110771179\n",
      "Epoch 79, CIFAR-10 Batch 1:  Loss: 0.0025871978141367435\n",
      "Accuracy: 0.7443997859954834\n",
      "Epoch 79, CIFAR-10 Batch 2:  Loss: 0.004417820833623409\n",
      "Accuracy: 0.7379999160766602\n",
      "Epoch 79, CIFAR-10 Batch 3:  Loss: 0.014313195832073689\n",
      "Accuracy: 0.746799886226654\n",
      "Epoch 79, CIFAR-10 Batch 4:  Loss: 0.019876733422279358\n",
      "Accuracy: 0.7195998430252075\n",
      "Epoch 79, CIFAR-10 Batch 5:  Loss: 0.006336516700685024\n",
      "Accuracy: 0.7433998584747314\n",
      "Epoch 80, CIFAR-10 Batch 1:  Loss: 0.004637741483747959\n",
      "Accuracy: 0.7415998578071594\n",
      "Epoch 80, CIFAR-10 Batch 2:  Loss: 0.010928967967629433\n",
      "Accuracy: 0.7345998287200928\n",
      "Epoch 80, CIFAR-10 Batch 3:  Loss: 0.013478247448801994\n",
      "Accuracy: 0.7307997941970825\n",
      "Epoch 80, CIFAR-10 Batch 4:  Loss: 0.003842896083369851\n",
      "Accuracy: 0.7485997676849365\n",
      "Epoch 80, CIFAR-10 Batch 5:  Loss: 0.004206954035907984\n",
      "Accuracy: 0.7415998578071594\n",
      "Epoch 81, CIFAR-10 Batch 1:  Loss: 0.002700735116377473\n",
      "Accuracy: 0.7463998794555664\n",
      "Epoch 81, CIFAR-10 Batch 2:  Loss: 0.008838717825710773\n",
      "Accuracy: 0.7423998713493347\n",
      "Epoch 81, CIFAR-10 Batch 3:  Loss: 0.004421448800712824\n",
      "Accuracy: 0.7335997819900513\n",
      "Epoch 81, CIFAR-10 Batch 4:  Loss: 0.002436453476548195\n",
      "Accuracy: 0.7455998063087463\n",
      "Epoch 81, CIFAR-10 Batch 5:  Loss: 0.0038363297935575247\n",
      "Accuracy: 0.7397998571395874\n",
      "Epoch 82, CIFAR-10 Batch 1:  Loss: 0.009330916218459606\n",
      "Accuracy: 0.7381998300552368\n",
      "Epoch 82, CIFAR-10 Batch 2:  Loss: 0.004025118425488472\n",
      "Accuracy: 0.741199791431427\n",
      "Epoch 82, CIFAR-10 Batch 3:  Loss: 0.012651682831346989\n",
      "Accuracy: 0.731799840927124\n",
      "Epoch 82, CIFAR-10 Batch 4:  Loss: 0.004829581826925278\n",
      "Accuracy: 0.7367998361587524\n",
      "Epoch 82, CIFAR-10 Batch 5:  Loss: 0.004989647306501865\n",
      "Accuracy: 0.7473998069763184\n",
      "Epoch 83, CIFAR-10 Batch 1:  Loss: 0.007805696688592434\n",
      "Accuracy: 0.740199863910675\n",
      "Epoch 83, CIFAR-10 Batch 2:  Loss: 0.014635596424341202\n",
      "Accuracy: 0.7435998320579529\n",
      "Epoch 83, CIFAR-10 Batch 3:  Loss: 0.0024925456382334232\n",
      "Accuracy: 0.7417998909950256\n",
      "Epoch 83, CIFAR-10 Batch 4:  Loss: 0.004379269666969776\n",
      "Accuracy: 0.7387998700141907\n",
      "Epoch 83, CIFAR-10 Batch 5:  Loss: 0.004889990668743849\n",
      "Accuracy: 0.7359998822212219\n",
      "Epoch 84, CIFAR-10 Batch 1:  Loss: 0.009912638925015926\n",
      "Accuracy: 0.7363998889923096\n",
      "Epoch 84, CIFAR-10 Batch 2:  Loss: 0.004507805220782757\n",
      "Accuracy: 0.7351998090744019\n",
      "Epoch 84, CIFAR-10 Batch 3:  Loss: 0.004886896349489689\n",
      "Accuracy: 0.7391997575759888\n",
      "Epoch 84, CIFAR-10 Batch 4:  Loss: 0.012606684118509293\n",
      "Accuracy: 0.739599883556366\n",
      "Epoch 84, CIFAR-10 Batch 5:  Loss: 0.003467091592028737\n",
      "Accuracy: 0.7363998889923096\n",
      "Epoch 85, CIFAR-10 Batch 1:  Loss: 0.0032575810328125954\n",
      "Accuracy: 0.7439998388290405\n",
      "Epoch 85, CIFAR-10 Batch 2:  Loss: 0.00438339076936245\n",
      "Accuracy: 0.7435998320579529\n",
      "Epoch 85, CIFAR-10 Batch 3:  Loss: 0.008303535170853138\n",
      "Accuracy: 0.7361998558044434\n",
      "Epoch 85, CIFAR-10 Batch 4:  Loss: 0.006264768540859222\n",
      "Accuracy: 0.7419998645782471\n",
      "Epoch 85, CIFAR-10 Batch 5:  Loss: 0.007293271832168102\n",
      "Accuracy: 0.7369998693466187\n",
      "Epoch 86, CIFAR-10 Batch 1:  Loss: 0.0031735675875097513\n",
      "Accuracy: 0.7393998503684998\n",
      "Epoch 86, CIFAR-10 Batch 2:  Loss: 0.0034923905041068792\n",
      "Accuracy: 0.7431998252868652\n",
      "Epoch 86, CIFAR-10 Batch 3:  Loss: 0.007689547259360552\n",
      "Accuracy: 0.7417998313903809\n",
      "Epoch 86, CIFAR-10 Batch 4:  Loss: 0.006051447708159685\n",
      "Accuracy: 0.740199863910675\n",
      "Epoch 86, CIFAR-10 Batch 5:  Loss: 0.015158816240727901\n",
      "Accuracy: 0.7343998551368713\n",
      "Epoch 87, CIFAR-10 Batch 1:  Loss: 0.005311302375048399\n",
      "Accuracy: 0.7503998279571533\n",
      "Epoch 87, CIFAR-10 Batch 2:  Loss: 0.01611199602484703\n",
      "Accuracy: 0.7371997833251953\n",
      "Epoch 87, CIFAR-10 Batch 3:  Loss: 0.009853871539235115\n",
      "Accuracy: 0.7339998483657837\n",
      "Epoch 87, CIFAR-10 Batch 4:  Loss: 0.005593433976173401\n",
      "Accuracy: 0.7445998787879944\n",
      "Epoch 87, CIFAR-10 Batch 5:  Loss: 0.008379237726330757\n",
      "Accuracy: 0.7413997650146484\n",
      "Epoch 88, CIFAR-10 Batch 1:  Loss: 0.008260850794613361\n",
      "Accuracy: 0.7385998964309692\n",
      "Epoch 88, CIFAR-10 Batch 2:  Loss: 0.00387774221599102\n",
      "Accuracy: 0.7395998239517212\n",
      "Epoch 88, CIFAR-10 Batch 3:  Loss: 0.017065050080418587\n",
      "Accuracy: 0.730199933052063\n",
      "Epoch 88, CIFAR-10 Batch 4:  Loss: 0.005688813515007496\n",
      "Accuracy: 0.7409998178482056\n",
      "Epoch 88, CIFAR-10 Batch 5:  Loss: 0.007310890592634678\n",
      "Accuracy: 0.7353998422622681\n",
      "Epoch 89, CIFAR-10 Batch 1:  Loss: 0.005462940316647291\n",
      "Accuracy: 0.7491998672485352\n",
      "Epoch 89, CIFAR-10 Batch 2:  Loss: 0.0026436676271259785\n",
      "Accuracy: 0.7451997995376587\n",
      "Epoch 89, CIFAR-10 Batch 3:  Loss: 0.006113989744335413\n",
      "Accuracy: 0.7395998239517212\n",
      "Epoch 89, CIFAR-10 Batch 4:  Loss: 0.004807822871953249\n",
      "Accuracy: 0.7411998510360718\n",
      "Epoch 89, CIFAR-10 Batch 5:  Loss: 0.0036120037548244\n",
      "Accuracy: 0.7479998469352722\n",
      "Epoch 90, CIFAR-10 Batch 1:  Loss: 0.003897362621501088\n",
      "Accuracy: 0.732799768447876\n",
      "Epoch 90, CIFAR-10 Batch 2:  Loss: 0.007438350468873978\n",
      "Accuracy: 0.7363998889923096\n",
      "Epoch 90, CIFAR-10 Batch 3:  Loss: 0.006223408039659262\n",
      "Accuracy: 0.740199863910675\n",
      "Epoch 90, CIFAR-10 Batch 4:  Loss: 0.0030006063170731068\n",
      "Accuracy: 0.7463998198509216\n",
      "Epoch 90, CIFAR-10 Batch 5:  Loss: 0.020136723294854164\n",
      "Accuracy: 0.7277998924255371\n",
      "Epoch 91, CIFAR-10 Batch 1:  Loss: 0.009444169700145721\n",
      "Accuracy: 0.7371999025344849\n",
      "Epoch 91, CIFAR-10 Batch 2:  Loss: 0.006891267839819193\n",
      "Accuracy: 0.734799861907959\n",
      "Epoch 91, CIFAR-10 Batch 3:  Loss: 0.0026830038987100124\n",
      "Accuracy: 0.7407998442649841\n",
      "Epoch 91, CIFAR-10 Batch 4:  Loss: 0.00910276174545288\n",
      "Accuracy: 0.741399884223938\n",
      "Epoch 91, CIFAR-10 Batch 5:  Loss: 0.0023788788821548223\n",
      "Accuracy: 0.7413998246192932\n",
      "Epoch 92, CIFAR-10 Batch 1:  Loss: 0.016421323642134666\n",
      "Accuracy: 0.7409998178482056\n",
      "Epoch 92, CIFAR-10 Batch 2:  Loss: 0.005070189945399761\n",
      "Accuracy: 0.7355998754501343\n",
      "Epoch 92, CIFAR-10 Batch 3:  Loss: 0.00717137148603797\n",
      "Accuracy: 0.7407997846603394\n",
      "Epoch 92, CIFAR-10 Batch 4:  Loss: 0.0036233484279364347\n",
      "Accuracy: 0.7527998685836792\n",
      "Epoch 92, CIFAR-10 Batch 5:  Loss: 0.00302923028357327\n",
      "Accuracy: 0.7439998984336853\n",
      "Epoch 93, CIFAR-10 Batch 1:  Loss: 0.009034868329763412\n",
      "Accuracy: 0.7369998097419739\n",
      "Epoch 93, CIFAR-10 Batch 2:  Loss: 0.0043545011430978775\n",
      "Accuracy: 0.7397998571395874\n",
      "Epoch 93, CIFAR-10 Batch 3:  Loss: 0.005582595709711313\n",
      "Accuracy: 0.7409998774528503\n",
      "Epoch 93, CIFAR-10 Batch 4:  Loss: 0.011317573487758636\n",
      "Accuracy: 0.7419998645782471\n",
      "Epoch 93, CIFAR-10 Batch 5:  Loss: 0.0024708942510187626\n",
      "Accuracy: 0.7379998564720154\n",
      "Epoch 94, CIFAR-10 Batch 1:  Loss: 0.008822549134492874\n",
      "Accuracy: 0.7365998029708862\n",
      "Epoch 94, CIFAR-10 Batch 2:  Loss: 0.004061984363943338\n",
      "Accuracy: 0.7423998713493347\n",
      "Epoch 94, CIFAR-10 Batch 3:  Loss: 0.008117808029055595\n",
      "Accuracy: 0.7465998530387878\n",
      "Epoch 94, CIFAR-10 Batch 4:  Loss: 0.0038453983142971992\n",
      "Accuracy: 0.7469998002052307\n",
      "Epoch 94, CIFAR-10 Batch 5:  Loss: 0.008864778093993664\n",
      "Accuracy: 0.7381998300552368\n",
      "Epoch 95, CIFAR-10 Batch 1:  Loss: 0.008085697889328003\n",
      "Accuracy: 0.7363998293876648\n",
      "Epoch 95, CIFAR-10 Batch 2:  Loss: 0.012736698612570763\n",
      "Accuracy: 0.734799861907959\n",
      "Epoch 95, CIFAR-10 Batch 3:  Loss: 0.003499479964375496\n",
      "Accuracy: 0.7443997859954834\n",
      "Epoch 95, CIFAR-10 Batch 4:  Loss: 0.005961787886917591\n",
      "Accuracy: 0.7397998571395874\n",
      "Epoch 95, CIFAR-10 Batch 5:  Loss: 0.0022783316671848297\n",
      "Accuracy: 0.7473998665809631\n",
      "Epoch 96, CIFAR-10 Batch 1:  Loss: 0.01119642611593008\n",
      "Accuracy: 0.7457998394966125\n",
      "Epoch 96, CIFAR-10 Batch 2:  Loss: 0.0032811807468533516\n",
      "Accuracy: 0.7395998239517212\n",
      "Epoch 96, CIFAR-10 Batch 3:  Loss: 0.002715979004278779\n",
      "Accuracy: 0.7463998794555664\n",
      "Epoch 96, CIFAR-10 Batch 4:  Loss: 0.002041606232523918\n",
      "Accuracy: 0.7439998388290405\n",
      "Epoch 96, CIFAR-10 Batch 5:  Loss: 0.0026397998444736004\n",
      "Accuracy: 0.7445998191833496\n",
      "Epoch 97, CIFAR-10 Batch 1:  Loss: 0.004112648777663708\n",
      "Accuracy: 0.7357999086380005\n",
      "Epoch 97, CIFAR-10 Batch 2:  Loss: 0.0014776226598769426\n",
      "Accuracy: 0.7389998435974121\n",
      "Epoch 97, CIFAR-10 Batch 3:  Loss: 0.001272385474294424\n",
      "Accuracy: 0.741399884223938\n",
      "Epoch 97, CIFAR-10 Batch 4:  Loss: 0.0033683152869343758\n",
      "Accuracy: 0.7399998307228088\n",
      "Epoch 97, CIFAR-10 Batch 5:  Loss: 0.004837593995034695\n",
      "Accuracy: 0.7451998591423035\n",
      "Epoch 98, CIFAR-10 Batch 1:  Loss: 0.003703324357047677\n",
      "Accuracy: 0.7331998348236084\n",
      "Epoch 98, CIFAR-10 Batch 2:  Loss: 0.002668223809450865\n",
      "Accuracy: 0.7419998049736023\n",
      "Epoch 98, CIFAR-10 Batch 3:  Loss: 0.008542755618691444\n",
      "Accuracy: 0.7419999241828918\n",
      "Epoch 98, CIFAR-10 Batch 4:  Loss: 0.003861701348796487\n",
      "Accuracy: 0.7463999390602112\n",
      "Epoch 98, CIFAR-10 Batch 5:  Loss: 0.007061311509460211\n",
      "Accuracy: 0.732999861240387\n",
      "Epoch 99, CIFAR-10 Batch 1:  Loss: 0.006926066242158413\n",
      "Accuracy: 0.7371999025344849\n",
      "Epoch 99, CIFAR-10 Batch 2:  Loss: 0.0029925203416496515\n",
      "Accuracy: 0.7385997772216797\n",
      "Epoch 99, CIFAR-10 Batch 3:  Loss: 0.005278954282402992\n",
      "Accuracy: 0.7393998503684998\n",
      "Epoch 99, CIFAR-10 Batch 4:  Loss: 0.004670647904276848\n",
      "Accuracy: 0.7377998232841492\n",
      "Epoch 99, CIFAR-10 Batch 5:  Loss: 0.003302985802292824\n",
      "Accuracy: 0.737799882888794\n",
      "Epoch 100, CIFAR-10 Batch 1:  Loss: 0.004877334926277399\n",
      "Accuracy: 0.7341998815536499\n",
      "Epoch 100, CIFAR-10 Batch 2:  Loss: 0.009723312221467495\n",
      "Accuracy: 0.7435998916625977\n",
      "Epoch 100, CIFAR-10 Batch 3:  Loss: 0.0040848213247954845\n",
      "Accuracy: 0.7269999384880066\n",
      "Epoch 100, CIFAR-10 Batch 4:  Loss: 0.007059215568006039\n",
      "Accuracy: 0.7387998104095459\n",
      "Epoch 100, CIFAR-10 Batch 5:  Loss: 0.003978075925260782\n",
      "Accuracy: 0.741199791431427\n",
      "Epoch 101, CIFAR-10 Batch 1:  Loss: 0.001599000534042716\n",
      "Accuracy: 0.7431998252868652\n",
      "Epoch 101, CIFAR-10 Batch 2:  Loss: 0.0011696527944877744\n",
      "Accuracy: 0.7437998652458191\n",
      "Epoch 101, CIFAR-10 Batch 3:  Loss: 0.0019389353692531586\n",
      "Accuracy: 0.7379998564720154\n",
      "Epoch 101, CIFAR-10 Batch 4:  Loss: 0.007462481036782265\n",
      "Accuracy: 0.7505998611450195\n",
      "Epoch 101, CIFAR-10 Batch 5:  Loss: 0.0030740490183234215\n",
      "Accuracy: 0.7463998198509216\n",
      "Epoch 102, CIFAR-10 Batch 1:  Loss: 0.0018052419181913137\n",
      "Accuracy: 0.7431998252868652\n",
      "Epoch 102, CIFAR-10 Batch 2:  Loss: 0.0028743508737534285\n",
      "Accuracy: 0.7411998510360718\n",
      "Epoch 102, CIFAR-10 Batch 3:  Loss: 0.003201535437256098\n",
      "Accuracy: 0.7371998429298401\n",
      "Epoch 102, CIFAR-10 Batch 4:  Loss: 0.0021396318916231394\n",
      "Accuracy: 0.7391998171806335\n",
      "Epoch 102, CIFAR-10 Batch 5:  Loss: 0.003966419957578182\n",
      "Accuracy: 0.739399790763855\n",
      "Epoch 103, CIFAR-10 Batch 1:  Loss: 0.004968770779669285\n",
      "Accuracy: 0.7377999424934387\n",
      "Epoch 103, CIFAR-10 Batch 2:  Loss: 0.0024108581710606813\n",
      "Accuracy: 0.7431997656822205\n",
      "Epoch 103, CIFAR-10 Batch 3:  Loss: 0.002636706456542015\n",
      "Accuracy: 0.7359997630119324\n",
      "Epoch 103, CIFAR-10 Batch 4:  Loss: 0.0016282564029097557\n",
      "Accuracy: 0.731799840927124\n",
      "Epoch 103, CIFAR-10 Batch 5:  Loss: 0.002560175023972988\n",
      "Accuracy: 0.7431997656822205\n",
      "Epoch 104, CIFAR-10 Batch 1:  Loss: 0.002411754336208105\n",
      "Accuracy: 0.7349998950958252\n",
      "Epoch 104, CIFAR-10 Batch 2:  Loss: 0.0032020171638578176\n",
      "Accuracy: 0.7455998659133911\n",
      "Epoch 104, CIFAR-10 Batch 3:  Loss: 0.0024725969415158033\n",
      "Accuracy: 0.7331998348236084\n",
      "Epoch 104, CIFAR-10 Batch 4:  Loss: 0.008086035028100014\n",
      "Accuracy: 0.7389998435974121\n",
      "Epoch 104, CIFAR-10 Batch 5:  Loss: 0.002221517963334918\n",
      "Accuracy: 0.7419998645782471\n",
      "Epoch 105, CIFAR-10 Batch 1:  Loss: 0.003716110484674573\n",
      "Accuracy: 0.7353999018669128\n",
      "Epoch 105, CIFAR-10 Batch 2:  Loss: 0.0038652033545076847\n",
      "Accuracy: 0.7363998293876648\n",
      "Epoch 105, CIFAR-10 Batch 3:  Loss: 0.002587371738627553\n",
      "Accuracy: 0.7247998714447021\n",
      "Epoch 105, CIFAR-10 Batch 4:  Loss: 0.004731690511107445\n",
      "Accuracy: 0.7353998422622681\n",
      "Epoch 105, CIFAR-10 Batch 5:  Loss: 0.0024776046629995108\n",
      "Accuracy: 0.7341998219490051\n",
      "Epoch 106, CIFAR-10 Batch 1:  Loss: 0.002647565910592675\n",
      "Accuracy: 0.7331998348236084\n",
      "Epoch 106, CIFAR-10 Batch 2:  Loss: 0.008876240812242031\n",
      "Accuracy: 0.7461998462677002\n",
      "Epoch 106, CIFAR-10 Batch 3:  Loss: 0.0012667987029999495\n",
      "Accuracy: 0.7333998680114746\n",
      "Epoch 106, CIFAR-10 Batch 4:  Loss: 0.007924615405499935\n",
      "Accuracy: 0.7437998056411743\n",
      "Epoch 106, CIFAR-10 Batch 5:  Loss: 0.002296054968610406\n",
      "Accuracy: 0.7309998273849487\n",
      "Epoch 107, CIFAR-10 Batch 1:  Loss: 0.004249496851116419\n",
      "Accuracy: 0.7355998754501343\n",
      "Epoch 107, CIFAR-10 Batch 2:  Loss: 0.003012185450643301\n",
      "Accuracy: 0.7413998246192932\n",
      "Epoch 107, CIFAR-10 Batch 3:  Loss: 0.0039024334400892258\n",
      "Accuracy: 0.7401998043060303\n",
      "Epoch 107, CIFAR-10 Batch 4:  Loss: 0.0026607054751366377\n",
      "Accuracy: 0.7431998252868652\n",
      "Epoch 107, CIFAR-10 Batch 5:  Loss: 0.0057663158513605595\n",
      "Accuracy: 0.7385998964309692\n",
      "Epoch 108, CIFAR-10 Batch 1:  Loss: 0.008920176886022091\n",
      "Accuracy: 0.7437998652458191\n",
      "Epoch 108, CIFAR-10 Batch 2:  Loss: 0.002410312881693244\n",
      "Accuracy: 0.7455998659133911\n",
      "Epoch 108, CIFAR-10 Batch 3:  Loss: 0.005285350605845451\n",
      "Accuracy: 0.7381998896598816\n",
      "Epoch 108, CIFAR-10 Batch 4:  Loss: 0.008362719789147377\n",
      "Accuracy: 0.7397998571395874\n",
      "Epoch 108, CIFAR-10 Batch 5:  Loss: 0.0022857366129755974\n",
      "Accuracy: 0.7369998097419739\n",
      "Epoch 109, CIFAR-10 Batch 1:  Loss: 0.0017285655485466123\n",
      "Accuracy: 0.7375998497009277\n",
      "Epoch 109, CIFAR-10 Batch 2:  Loss: 0.002734152367338538\n",
      "Accuracy: 0.7465997934341431\n",
      "Epoch 109, CIFAR-10 Batch 3:  Loss: 0.008826474659144878\n",
      "Accuracy: 0.7399998903274536\n",
      "Epoch 109, CIFAR-10 Batch 4:  Loss: 0.003205734770745039\n",
      "Accuracy: 0.7451998591423035\n",
      "Epoch 109, CIFAR-10 Batch 5:  Loss: 0.002017856575548649\n",
      "Accuracy: 0.7503998279571533\n",
      "Epoch 110, CIFAR-10 Batch 1:  Loss: 0.004776092246174812\n",
      "Accuracy: 0.7407998442649841\n",
      "Epoch 110, CIFAR-10 Batch 2:  Loss: 0.002338112099096179\n",
      "Accuracy: 0.7439998388290405\n",
      "Epoch 110, CIFAR-10 Batch 3:  Loss: 0.0036881796550005674\n",
      "Accuracy: 0.7325998544692993\n",
      "Epoch 110, CIFAR-10 Batch 4:  Loss: 0.0011475346982479095\n",
      "Accuracy: 0.7451998591423035\n",
      "Epoch 110, CIFAR-10 Batch 5:  Loss: 0.0018049972131848335\n",
      "Accuracy: 0.7403998374938965\n",
      "Epoch 111, CIFAR-10 Batch 1:  Loss: 0.004266784060746431\n",
      "Accuracy: 0.7421998381614685\n",
      "Epoch 111, CIFAR-10 Batch 2:  Loss: 0.00115015113260597\n",
      "Accuracy: 0.7495997548103333\n",
      "Epoch 111, CIFAR-10 Batch 3:  Loss: 0.0021568427328020334\n",
      "Accuracy: 0.7351998686790466\n",
      "Epoch 111, CIFAR-10 Batch 4:  Loss: 0.005114695522934198\n",
      "Accuracy: 0.7391998767852783\n",
      "Epoch 111, CIFAR-10 Batch 5:  Loss: 0.0011768363183364272\n",
      "Accuracy: 0.7353999018669128\n",
      "Epoch 112, CIFAR-10 Batch 1:  Loss: 0.0008205380290746689\n",
      "Accuracy: 0.7375998497009277\n",
      "Epoch 112, CIFAR-10 Batch 2:  Loss: 0.00478527694940567\n",
      "Accuracy: 0.7301998138427734\n",
      "Epoch 112, CIFAR-10 Batch 3:  Loss: 0.00316190579906106\n",
      "Accuracy: 0.7263997793197632\n",
      "Epoch 112, CIFAR-10 Batch 4:  Loss: 0.0028225912246853113\n",
      "Accuracy: 0.7345998883247375\n",
      "Epoch 112, CIFAR-10 Batch 5:  Loss: 0.0014483400154858828\n",
      "Accuracy: 0.7387998104095459\n",
      "Epoch 113, CIFAR-10 Batch 1:  Loss: 0.0011259443126618862\n",
      "Accuracy: 0.7469998598098755\n",
      "Epoch 113, CIFAR-10 Batch 2:  Loss: 0.002073253970593214\n",
      "Accuracy: 0.7395998239517212\n",
      "Epoch 113, CIFAR-10 Batch 3:  Loss: 0.001635691849514842\n",
      "Accuracy: 0.732999861240387\n",
      "Epoch 113, CIFAR-10 Batch 4:  Loss: 0.0044586146250367165\n",
      "Accuracy: 0.7291997671127319\n",
      "Epoch 113, CIFAR-10 Batch 5:  Loss: 0.002861756831407547\n",
      "Accuracy: 0.7381998300552368\n",
      "Epoch 114, CIFAR-10 Batch 1:  Loss: 0.00224013882689178\n",
      "Accuracy: 0.7405998706817627\n",
      "Epoch 114, CIFAR-10 Batch 2:  Loss: 0.002020713407546282\n",
      "Accuracy: 0.7435998320579529\n",
      "Epoch 114, CIFAR-10 Batch 3:  Loss: 0.0016819594893604517\n",
      "Accuracy: 0.7389999032020569\n",
      "Epoch 114, CIFAR-10 Batch 4:  Loss: 0.006184605415910482\n",
      "Accuracy: 0.7291998267173767\n",
      "Epoch 114, CIFAR-10 Batch 5:  Loss: 0.004115606192499399\n",
      "Accuracy: 0.7375998497009277\n",
      "Epoch 115, CIFAR-10 Batch 1:  Loss: 0.003248556051403284\n",
      "Accuracy: 0.7391998767852783\n",
      "Epoch 115, CIFAR-10 Batch 2:  Loss: 0.0033625930082052946\n",
      "Accuracy: 0.7443998456001282\n",
      "Epoch 115, CIFAR-10 Batch 3:  Loss: 0.01595410145819187\n",
      "Accuracy: 0.7339999079704285\n",
      "Epoch 115, CIFAR-10 Batch 4:  Loss: 0.0022089399863034487\n",
      "Accuracy: 0.7411998510360718\n",
      "Epoch 115, CIFAR-10 Batch 5:  Loss: 0.007954377681016922\n",
      "Accuracy: 0.746799886226654\n",
      "Epoch 116, CIFAR-10 Batch 1:  Loss: 0.003525328356772661\n",
      "Accuracy: 0.7399998903274536\n",
      "Epoch 116, CIFAR-10 Batch 2:  Loss: 0.0042525650933384895\n",
      "Accuracy: 0.7375998497009277\n",
      "Epoch 116, CIFAR-10 Batch 3:  Loss: 0.005675827618688345\n",
      "Accuracy: 0.7425997853279114\n",
      "Epoch 116, CIFAR-10 Batch 4:  Loss: 0.0017705701757222414\n",
      "Accuracy: 0.7365998029708862\n",
      "Epoch 116, CIFAR-10 Batch 5:  Loss: 0.0028008052613586187\n",
      "Accuracy: 0.744399905204773\n",
      "Epoch 117, CIFAR-10 Batch 1:  Loss: 0.011660728603601456\n",
      "Accuracy: 0.739599883556366\n",
      "Epoch 117, CIFAR-10 Batch 2:  Loss: 0.009489412419497967\n",
      "Accuracy: 0.7409998178482056\n",
      "Epoch 117, CIFAR-10 Batch 3:  Loss: 0.0015475281979888678\n",
      "Accuracy: 0.7451997995376587\n",
      "Epoch 117, CIFAR-10 Batch 4:  Loss: 0.0021581193432211876\n",
      "Accuracy: 0.751599907875061\n",
      "Epoch 117, CIFAR-10 Batch 5:  Loss: 0.0034018426667898893\n",
      "Accuracy: 0.7455998659133911\n",
      "Epoch 118, CIFAR-10 Batch 1:  Loss: 0.002246493473649025\n",
      "Accuracy: 0.7437998056411743\n",
      "Epoch 118, CIFAR-10 Batch 2:  Loss: 0.005262607708573341\n",
      "Accuracy: 0.7405998706817627\n",
      "Epoch 118, CIFAR-10 Batch 3:  Loss: 0.02067888155579567\n",
      "Accuracy: 0.7363998293876648\n",
      "Epoch 118, CIFAR-10 Batch 4:  Loss: 0.005064621567726135\n",
      "Accuracy: 0.7403998374938965\n",
      "Epoch 118, CIFAR-10 Batch 5:  Loss: 0.0037642833776772022\n",
      "Accuracy: 0.7399997711181641\n",
      "Epoch 119, CIFAR-10 Batch 1:  Loss: 0.003331380197778344\n",
      "Accuracy: 0.7461998462677002\n",
      "Epoch 119, CIFAR-10 Batch 2:  Loss: 0.001461993670091033\n",
      "Accuracy: 0.7441998720169067\n",
      "Epoch 119, CIFAR-10 Batch 3:  Loss: 0.000838617270346731\n",
      "Accuracy: 0.7395998239517212\n",
      "Epoch 119, CIFAR-10 Batch 4:  Loss: 0.0014327318640425801\n",
      "Accuracy: 0.7469998598098755\n",
      "Epoch 119, CIFAR-10 Batch 5:  Loss: 0.003938604611903429\n",
      "Accuracy: 0.7303998470306396\n",
      "Epoch 120, CIFAR-10 Batch 1:  Loss: 0.00922425091266632\n",
      "Accuracy: 0.7395998239517212\n",
      "Epoch 120, CIFAR-10 Batch 2:  Loss: 0.0015729449223726988\n",
      "Accuracy: 0.7409998178482056\n",
      "Epoch 120, CIFAR-10 Batch 3:  Loss: 0.006554007064551115\n",
      "Accuracy: 0.7333998680114746\n",
      "Epoch 120, CIFAR-10 Batch 4:  Loss: 0.0018410903867334127\n",
      "Accuracy: 0.7475998401641846\n",
      "Epoch 120, CIFAR-10 Batch 5:  Loss: 0.005490634590387344\n",
      "Accuracy: 0.7425998449325562\n",
      "Epoch 121, CIFAR-10 Batch 1:  Loss: 0.010448259301483631\n",
      "Accuracy: 0.7435997724533081\n",
      "Epoch 121, CIFAR-10 Batch 2:  Loss: 0.008396171033382416\n",
      "Accuracy: 0.7349998950958252\n",
      "Epoch 121, CIFAR-10 Batch 3:  Loss: 0.0016356626292690635\n",
      "Accuracy: 0.7389997839927673\n",
      "Epoch 121, CIFAR-10 Batch 4:  Loss: 0.0023528446909040213\n",
      "Accuracy: 0.7403998374938965\n",
      "Epoch 121, CIFAR-10 Batch 5:  Loss: 0.01233713235706091\n",
      "Accuracy: 0.7429998517036438\n",
      "Epoch 122, CIFAR-10 Batch 1:  Loss: 0.0014320580521598458\n",
      "Accuracy: 0.7357998490333557\n",
      "Epoch 122, CIFAR-10 Batch 2:  Loss: 0.0031704725697636604\n",
      "Accuracy: 0.7341998815536499\n",
      "Epoch 122, CIFAR-10 Batch 3:  Loss: 0.0013114716857671738\n",
      "Accuracy: 0.7417998909950256\n",
      "Epoch 122, CIFAR-10 Batch 4:  Loss: 0.0013517130864784122\n",
      "Accuracy: 0.7515997886657715\n",
      "Epoch 122, CIFAR-10 Batch 5:  Loss: 0.0035727391950786114\n",
      "Accuracy: 0.740199863910675\n",
      "Epoch 123, CIFAR-10 Batch 1:  Loss: 0.0021811595652252436\n",
      "Accuracy: 0.7387998104095459\n",
      "Epoch 123, CIFAR-10 Batch 2:  Loss: 0.005047244019806385\n",
      "Accuracy: 0.7501997947692871\n",
      "Epoch 123, CIFAR-10 Batch 3:  Loss: 0.002628857269883156\n",
      "Accuracy: 0.7351999282836914\n",
      "Epoch 123, CIFAR-10 Batch 4:  Loss: 0.001704716356471181\n",
      "Accuracy: 0.746799886226654\n",
      "Epoch 123, CIFAR-10 Batch 5:  Loss: 0.0012838604161515832\n",
      "Accuracy: 0.7517998218536377\n",
      "Epoch 124, CIFAR-10 Batch 1:  Loss: 0.0013983170501887798\n",
      "Accuracy: 0.7425998449325562\n",
      "Epoch 124, CIFAR-10 Batch 2:  Loss: 0.004112706985324621\n",
      "Accuracy: 0.7373998761177063\n",
      "Epoch 124, CIFAR-10 Batch 3:  Loss: 0.0037707057781517506\n",
      "Accuracy: 0.7183998823165894\n",
      "Epoch 124, CIFAR-10 Batch 4:  Loss: 0.0028581349179148674\n",
      "Accuracy: 0.7405998110771179\n",
      "Epoch 124, CIFAR-10 Batch 5:  Loss: 0.006305944174528122\n",
      "Accuracy: 0.7471998929977417\n",
      "Epoch 125, CIFAR-10 Batch 1:  Loss: 0.0019006055081263185\n",
      "Accuracy: 0.7409998774528503\n",
      "Epoch 125, CIFAR-10 Batch 2:  Loss: 0.0018057660199701786\n",
      "Accuracy: 0.744399905204773\n",
      "Epoch 125, CIFAR-10 Batch 3:  Loss: 0.0061640129424631596\n",
      "Accuracy: 0.7355997562408447\n",
      "Epoch 125, CIFAR-10 Batch 4:  Loss: 0.004212128929793835\n",
      "Accuracy: 0.7467998266220093\n",
      "Epoch 125, CIFAR-10 Batch 5:  Loss: 0.0004910427378490567\n",
      "Accuracy: 0.7471998333930969\n",
      "Epoch 126, CIFAR-10 Batch 1:  Loss: 0.0009429955389350653\n",
      "Accuracy: 0.7427998781204224\n",
      "Epoch 126, CIFAR-10 Batch 2:  Loss: 0.005306873004883528\n",
      "Accuracy: 0.7423997521400452\n",
      "Epoch 126, CIFAR-10 Batch 3:  Loss: 0.003993752412497997\n",
      "Accuracy: 0.7399998307228088\n",
      "Epoch 126, CIFAR-10 Batch 4:  Loss: 0.003908481448888779\n",
      "Accuracy: 0.7483998537063599\n",
      "Epoch 126, CIFAR-10 Batch 5:  Loss: 0.0019201156683266163\n",
      "Accuracy: 0.7423998713493347\n",
      "Epoch 127, CIFAR-10 Batch 1:  Loss: 0.008157429285347462\n",
      "Accuracy: 0.7227998375892639\n",
      "Epoch 127, CIFAR-10 Batch 2:  Loss: 0.0063589876517653465\n",
      "Accuracy: 0.7405998110771179\n",
      "Epoch 127, CIFAR-10 Batch 3:  Loss: 0.0011766786919906735\n",
      "Accuracy: 0.7403998374938965\n",
      "Epoch 127, CIFAR-10 Batch 4:  Loss: 0.0035250133369117975\n",
      "Accuracy: 0.748999834060669\n",
      "Epoch 127, CIFAR-10 Batch 5:  Loss: 0.0030578961595892906\n",
      "Accuracy: 0.744399905204773\n",
      "Epoch 128, CIFAR-10 Batch 1:  Loss: 0.002179579110816121\n",
      "Accuracy: 0.7461997866630554\n",
      "Epoch 128, CIFAR-10 Batch 2:  Loss: 0.002303977496922016\n",
      "Accuracy: 0.7435998320579529\n",
      "Epoch 128, CIFAR-10 Batch 3:  Loss: 0.0027698082849383354\n",
      "Accuracy: 0.7389997839927673\n",
      "Epoch 128, CIFAR-10 Batch 4:  Loss: 0.0030280635692179203\n",
      "Accuracy: 0.7423998713493347\n",
      "Epoch 128, CIFAR-10 Batch 5:  Loss: 0.00548275001347065\n",
      "Accuracy: 0.7431998252868652\n",
      "Epoch 129, CIFAR-10 Batch 1:  Loss: 0.0037086422089487314\n",
      "Accuracy: 0.7453997731208801\n",
      "Epoch 129, CIFAR-10 Batch 2:  Loss: 0.0005629169754683971\n",
      "Accuracy: 0.7465998530387878\n",
      "Epoch 129, CIFAR-10 Batch 3:  Loss: 0.0011371028376743197\n",
      "Accuracy: 0.7505998611450195\n",
      "Epoch 129, CIFAR-10 Batch 4:  Loss: 0.0009419102570973337\n",
      "Accuracy: 0.7511997818946838\n",
      "Epoch 129, CIFAR-10 Batch 5:  Loss: 0.004334297496825457\n",
      "Accuracy: 0.7515997886657715\n",
      "Epoch 130, CIFAR-10 Batch 1:  Loss: 0.0016293682856485248\n",
      "Accuracy: 0.7429999113082886\n",
      "Epoch 130, CIFAR-10 Batch 2:  Loss: 0.00766834057867527\n",
      "Accuracy: 0.7333998680114746\n",
      "Epoch 130, CIFAR-10 Batch 3:  Loss: 0.0013964114477857947\n",
      "Accuracy: 0.7371997833251953\n",
      "Epoch 130, CIFAR-10 Batch 4:  Loss: 0.004214450716972351\n",
      "Accuracy: 0.7403998374938965\n",
      "Epoch 130, CIFAR-10 Batch 5:  Loss: 0.0020002962555736303\n",
      "Accuracy: 0.7367998957633972\n",
      "Epoch 131, CIFAR-10 Batch 1:  Loss: 0.0062407199293375015\n",
      "Accuracy: 0.7433998584747314\n",
      "Epoch 131, CIFAR-10 Batch 2:  Loss: 0.005854198709130287\n",
      "Accuracy: 0.7387999296188354\n",
      "Epoch 131, CIFAR-10 Batch 3:  Loss: 0.006990049500018358\n",
      "Accuracy: 0.7335997819900513\n",
      "Epoch 131, CIFAR-10 Batch 4:  Loss: 0.0010140608064830303\n",
      "Accuracy: 0.7461998462677002\n",
      "Epoch 131, CIFAR-10 Batch 5:  Loss: 0.000701253185980022\n",
      "Accuracy: 0.7501997947692871\n",
      "Epoch 132, CIFAR-10 Batch 1:  Loss: 0.010238472372293472\n",
      "Accuracy: 0.7401998043060303\n",
      "Epoch 132, CIFAR-10 Batch 2:  Loss: 0.002889441791921854\n",
      "Accuracy: 0.7315998673439026\n",
      "Epoch 132, CIFAR-10 Batch 3:  Loss: 0.0006408500485122204\n",
      "Accuracy: 0.7425997853279114\n",
      "Epoch 132, CIFAR-10 Batch 4:  Loss: 0.004326177295297384\n",
      "Accuracy: 0.7439998388290405\n",
      "Epoch 132, CIFAR-10 Batch 5:  Loss: 0.007120614405721426\n",
      "Accuracy: 0.7469998598098755\n",
      "Epoch 133, CIFAR-10 Batch 1:  Loss: 0.004948672838509083\n",
      "Accuracy: 0.7427998781204224\n",
      "Epoch 133, CIFAR-10 Batch 2:  Loss: 0.0020186908077448606\n",
      "Accuracy: 0.7479998469352722\n",
      "Epoch 133, CIFAR-10 Batch 3:  Loss: 0.0009593924041837454\n",
      "Accuracy: 0.740199863910675\n",
      "Epoch 133, CIFAR-10 Batch 4:  Loss: 0.003305015154182911\n",
      "Accuracy: 0.746799886226654\n",
      "Epoch 133, CIFAR-10 Batch 5:  Loss: 0.003417381551116705\n",
      "Accuracy: 0.7493998408317566\n",
      "Epoch 134, CIFAR-10 Batch 1:  Loss: 0.003065627533942461\n",
      "Accuracy: 0.7429999113082886\n",
      "Epoch 134, CIFAR-10 Batch 2:  Loss: 0.0015932177193462849\n",
      "Accuracy: 0.7415999174118042\n",
      "Epoch 134, CIFAR-10 Batch 3:  Loss: 0.002260540146380663\n",
      "Accuracy: 0.7389998435974121\n",
      "Epoch 134, CIFAR-10 Batch 4:  Loss: 0.007948777638375759\n",
      "Accuracy: 0.7405998110771179\n",
      "Epoch 134, CIFAR-10 Batch 5:  Loss: 0.002288971794769168\n",
      "Accuracy: 0.7501997947692871\n",
      "Epoch 135, CIFAR-10 Batch 1:  Loss: 0.001653339248150587\n",
      "Accuracy: 0.7483998537063599\n",
      "Epoch 135, CIFAR-10 Batch 2:  Loss: 0.002702252473682165\n",
      "Accuracy: 0.7449997663497925\n",
      "Epoch 135, CIFAR-10 Batch 3:  Loss: 0.0020754935685545206\n",
      "Accuracy: 0.7415998578071594\n",
      "Epoch 135, CIFAR-10 Batch 4:  Loss: 0.0034111752174794674\n",
      "Accuracy: 0.7453998923301697\n",
      "Epoch 135, CIFAR-10 Batch 5:  Loss: 0.0021607400849461555\n",
      "Accuracy: 0.7433998584747314\n",
      "Epoch 136, CIFAR-10 Batch 1:  Loss: 0.0012465132167562842\n",
      "Accuracy: 0.7415997982025146\n",
      "Epoch 136, CIFAR-10 Batch 2:  Loss: 0.0010092563461512327\n",
      "Accuracy: 0.7449997663497925\n",
      "Epoch 136, CIFAR-10 Batch 3:  Loss: 0.011347578838467598\n",
      "Accuracy: 0.7473998069763184\n",
      "Epoch 136, CIFAR-10 Batch 4:  Loss: 0.0007693873485550284\n",
      "Accuracy: 0.747999906539917\n",
      "Epoch 136, CIFAR-10 Batch 5:  Loss: 0.0037330673076212406\n",
      "Accuracy: 0.757599949836731\n",
      "Epoch 137, CIFAR-10 Batch 1:  Loss: 0.0050749946385622025\n",
      "Accuracy: 0.7337998151779175\n",
      "Epoch 137, CIFAR-10 Batch 2:  Loss: 0.0076448204927146435\n",
      "Accuracy: 0.7337998747825623\n",
      "Epoch 137, CIFAR-10 Batch 3:  Loss: 0.0023998438846319914\n",
      "Accuracy: 0.7359998822212219\n",
      "Epoch 137, CIFAR-10 Batch 4:  Loss: 0.005849364213645458\n",
      "Accuracy: 0.739399790763855\n",
      "Epoch 137, CIFAR-10 Batch 5:  Loss: 0.0032020779326558113\n",
      "Accuracy: 0.7519998550415039\n",
      "Epoch 138, CIFAR-10 Batch 1:  Loss: 0.0007482722285203636\n",
      "Accuracy: 0.7387998700141907\n",
      "Epoch 138, CIFAR-10 Batch 2:  Loss: 0.0008042725967243314\n",
      "Accuracy: 0.729999840259552\n",
      "Epoch 138, CIFAR-10 Batch 3:  Loss: 0.005395072512328625\n",
      "Accuracy: 0.744799792766571\n",
      "Epoch 138, CIFAR-10 Batch 4:  Loss: 0.002117588883265853\n",
      "Accuracy: 0.7509998679161072\n",
      "Epoch 138, CIFAR-10 Batch 5:  Loss: 0.000560712767764926\n",
      "Accuracy: 0.7465998530387878\n",
      "Epoch 139, CIFAR-10 Batch 1:  Loss: 0.0014911200851202011\n",
      "Accuracy: 0.7445998191833496\n",
      "Epoch 139, CIFAR-10 Batch 2:  Loss: 0.006662047002464533\n",
      "Accuracy: 0.7379999160766602\n",
      "Epoch 139, CIFAR-10 Batch 3:  Loss: 0.003041996853426099\n",
      "Accuracy: 0.740199863910675\n",
      "Epoch 139, CIFAR-10 Batch 4:  Loss: 0.0009515678393654525\n",
      "Accuracy: 0.7445998787879944\n",
      "Epoch 139, CIFAR-10 Batch 5:  Loss: 0.0036305387038737535\n",
      "Accuracy: 0.744199812412262\n",
      "Epoch 140, CIFAR-10 Batch 1:  Loss: 0.0016373140970245004\n",
      "Accuracy: 0.7407997846603394\n",
      "Epoch 140, CIFAR-10 Batch 2:  Loss: 0.0024960492737591267\n",
      "Accuracy: 0.7413998246192932\n",
      "Epoch 140, CIFAR-10 Batch 3:  Loss: 0.0023739077150821686\n",
      "Accuracy: 0.734799861907959\n",
      "Epoch 140, CIFAR-10 Batch 4:  Loss: 0.0037670994643121958\n",
      "Accuracy: 0.742999792098999\n",
      "Epoch 140, CIFAR-10 Batch 5:  Loss: 0.0035639863926917315\n",
      "Accuracy: 0.7457998394966125\n",
      "Epoch 141, CIFAR-10 Batch 1:  Loss: 0.0031463552732020617\n",
      "Accuracy: 0.741199791431427\n",
      "Epoch 141, CIFAR-10 Batch 2:  Loss: 0.0009375159279443324\n",
      "Accuracy: 0.7423998713493347\n",
      "Epoch 141, CIFAR-10 Batch 3:  Loss: 0.0002455963403917849\n",
      "Accuracy: 0.7491998076438904\n",
      "Epoch 141, CIFAR-10 Batch 4:  Loss: 0.001028736005537212\n",
      "Accuracy: 0.745999813079834\n",
      "Epoch 141, CIFAR-10 Batch 5:  Loss: 0.00276360590942204\n",
      "Accuracy: 0.7571998834609985\n",
      "Epoch 142, CIFAR-10 Batch 1:  Loss: 0.000772377650719136\n",
      "Accuracy: 0.7399998903274536\n",
      "Epoch 142, CIFAR-10 Batch 2:  Loss: 0.0008503119461238384\n",
      "Accuracy: 0.7363998293876648\n",
      "Epoch 142, CIFAR-10 Batch 3:  Loss: 0.005946028977632523\n",
      "Accuracy: 0.7439998388290405\n",
      "Epoch 142, CIFAR-10 Batch 4:  Loss: 0.0026331592816859484\n",
      "Accuracy: 0.7477998733520508\n",
      "Epoch 142, CIFAR-10 Batch 5:  Loss: 0.002324859146028757\n",
      "Accuracy: 0.7529997825622559\n",
      "Epoch 143, CIFAR-10 Batch 1:  Loss: 0.0013387021608650684\n",
      "Accuracy: 0.7439998388290405\n",
      "Epoch 143, CIFAR-10 Batch 2:  Loss: 0.0034443987533450127\n",
      "Accuracy: 0.7357997894287109\n",
      "Epoch 143, CIFAR-10 Batch 3:  Loss: 0.0018226238898932934\n",
      "Accuracy: 0.7357998490333557\n",
      "Epoch 143, CIFAR-10 Batch 4:  Loss: 0.0019139749929308891\n",
      "Accuracy: 0.7461998462677002\n",
      "Epoch 143, CIFAR-10 Batch 5:  Loss: 0.0016289179911836982\n",
      "Accuracy: 0.752599835395813\n",
      "Epoch 144, CIFAR-10 Batch 1:  Loss: 0.0031067808158695698\n",
      "Accuracy: 0.7351997494697571\n",
      "Epoch 144, CIFAR-10 Batch 2:  Loss: 0.0010220113908872008\n",
      "Accuracy: 0.7453998327255249\n",
      "Epoch 144, CIFAR-10 Batch 3:  Loss: 0.006080960854887962\n",
      "Accuracy: 0.7433997988700867\n",
      "Epoch 144, CIFAR-10 Batch 4:  Loss: 0.003223235020413995\n",
      "Accuracy: 0.7409998774528503\n",
      "Epoch 144, CIFAR-10 Batch 5:  Loss: 0.0017925647553056479\n",
      "Accuracy: 0.7487998604774475\n",
      "Epoch 145, CIFAR-10 Batch 1:  Loss: 0.0007579930243082345\n",
      "Accuracy: 0.7383998036384583\n",
      "Epoch 145, CIFAR-10 Batch 2:  Loss: 0.0006648193229921162\n",
      "Accuracy: 0.7381998300552368\n",
      "Epoch 145, CIFAR-10 Batch 3:  Loss: 0.0007645636214874685\n",
      "Accuracy: 0.7391998767852783\n",
      "Epoch 145, CIFAR-10 Batch 4:  Loss: 0.0036704903468489647\n",
      "Accuracy: 0.7421998381614685\n",
      "Epoch 145, CIFAR-10 Batch 5:  Loss: 0.0004175285866949707\n",
      "Accuracy: 0.7391998171806335\n",
      "Epoch 146, CIFAR-10 Batch 1:  Loss: 0.0033181048929691315\n",
      "Accuracy: 0.7527998685836792\n",
      "Epoch 146, CIFAR-10 Batch 2:  Loss: 0.0009519830928184092\n",
      "Accuracy: 0.7509998679161072\n",
      "Epoch 146, CIFAR-10 Batch 3:  Loss: 0.002169230254366994\n",
      "Accuracy: 0.7405998706817627\n",
      "Epoch 146, CIFAR-10 Batch 4:  Loss: 0.0025431241374462843\n",
      "Accuracy: 0.7453998327255249\n",
      "Epoch 146, CIFAR-10 Batch 5:  Loss: 0.0008070953190326691\n",
      "Accuracy: 0.7523998022079468\n",
      "Epoch 147, CIFAR-10 Batch 1:  Loss: 0.0016731275245547295\n",
      "Accuracy: 0.749599814414978\n",
      "Epoch 147, CIFAR-10 Batch 2:  Loss: 0.003587068058550358\n",
      "Accuracy: 0.7475997805595398\n",
      "Epoch 147, CIFAR-10 Batch 3:  Loss: 0.0007615848444402218\n",
      "Accuracy: 0.7455998659133911\n",
      "Epoch 147, CIFAR-10 Batch 4:  Loss: 0.0006276870262809098\n",
      "Accuracy: 0.7487998604774475\n",
      "Epoch 147, CIFAR-10 Batch 5:  Loss: 0.001145287649706006\n",
      "Accuracy: 0.7509998083114624\n",
      "Epoch 148, CIFAR-10 Batch 1:  Loss: 0.0021243884693831205\n",
      "Accuracy: 0.7453998327255249\n",
      "Epoch 148, CIFAR-10 Batch 2:  Loss: 0.0026752690318971872\n",
      "Accuracy: 0.7329998016357422\n",
      "Epoch 148, CIFAR-10 Batch 3:  Loss: 0.00034725581645034254\n",
      "Accuracy: 0.7447998523712158\n",
      "Epoch 148, CIFAR-10 Batch 4:  Loss: 0.0018978691659867764\n",
      "Accuracy: 0.7539998888969421\n",
      "Epoch 148, CIFAR-10 Batch 5:  Loss: 0.005218966398388147\n",
      "Accuracy: 0.7409998178482056\n",
      "Epoch 149, CIFAR-10 Batch 1:  Loss: 0.005166212562471628\n",
      "Accuracy: 0.7399998307228088\n",
      "Epoch 149, CIFAR-10 Batch 2:  Loss: 0.0016390522941946983\n",
      "Accuracy: 0.7445998191833496\n",
      "Epoch 149, CIFAR-10 Batch 3:  Loss: 0.0007315381662920117\n",
      "Accuracy: 0.7461998462677002\n",
      "Epoch 149, CIFAR-10 Batch 4:  Loss: 0.0006388216861523688\n",
      "Accuracy: 0.748999834060669\n",
      "Epoch 149, CIFAR-10 Batch 5:  Loss: 0.0005897378432564437\n",
      "Accuracy: 0.7599998116493225\n",
      "Epoch 150, CIFAR-10 Batch 1:  Loss: 0.012989480048418045\n",
      "Accuracy: 0.7427998781204224\n",
      "Epoch 150, CIFAR-10 Batch 2:  Loss: 0.004531173966825008\n",
      "Accuracy: 0.7421997785568237\n",
      "Epoch 150, CIFAR-10 Batch 3:  Loss: 0.0035782628692686558\n",
      "Accuracy: 0.739599883556366\n",
      "Epoch 150, CIFAR-10 Batch 4:  Loss: 0.011763685382902622\n",
      "Accuracy: 0.7413997650146484\n",
      "Epoch 150, CIFAR-10 Batch 5:  Loss: 0.0011839544167742133\n",
      "Accuracy: 0.7499998807907104\n",
      "Epoch 151, CIFAR-10 Batch 1:  Loss: 0.0012085378402844071\n",
      "Accuracy: 0.7509998083114624\n",
      "Epoch 151, CIFAR-10 Batch 2:  Loss: 0.0007671494968235493\n",
      "Accuracy: 0.7447998523712158\n",
      "Epoch 151, CIFAR-10 Batch 3:  Loss: 0.0010040635243058205\n",
      "Accuracy: 0.747999906539917\n",
      "Epoch 151, CIFAR-10 Batch 4:  Loss: 0.0037472350522875786\n",
      "Accuracy: 0.7457998394966125\n",
      "Epoch 151, CIFAR-10 Batch 5:  Loss: 0.017093371599912643\n",
      "Accuracy: 0.7399998307228088\n",
      "Epoch 152, CIFAR-10 Batch 1:  Loss: 0.001649979967623949\n",
      "Accuracy: 0.7417998313903809\n",
      "Epoch 152, CIFAR-10 Batch 2:  Loss: 0.00233460427261889\n",
      "Accuracy: 0.7485998868942261\n",
      "Epoch 152, CIFAR-10 Batch 3:  Loss: 0.002506713382899761\n",
      "Accuracy: 0.7331998348236084\n",
      "Epoch 152, CIFAR-10 Batch 4:  Loss: 0.0022097774781286716\n",
      "Accuracy: 0.7425998449325562\n",
      "Epoch 152, CIFAR-10 Batch 5:  Loss: 0.0025939762126654387\n",
      "Accuracy: 0.749599814414978\n",
      "Epoch 153, CIFAR-10 Batch 1:  Loss: 0.0011539249680936337\n",
      "Accuracy: 0.7517998218536377\n",
      "Epoch 153, CIFAR-10 Batch 2:  Loss: 0.0074341073632240295\n",
      "Accuracy: 0.737599790096283\n",
      "Epoch 153, CIFAR-10 Batch 3:  Loss: 0.0007759045693092048\n",
      "Accuracy: 0.7479997873306274\n",
      "Epoch 153, CIFAR-10 Batch 4:  Loss: 0.011170067824423313\n",
      "Accuracy: 0.7497998476028442\n",
      "Epoch 153, CIFAR-10 Batch 5:  Loss: 0.0011638544965535402\n",
      "Accuracy: 0.7503998279571533\n",
      "Epoch 154, CIFAR-10 Batch 1:  Loss: 0.0030147333163768053\n",
      "Accuracy: 0.7437998652458191\n",
      "Epoch 154, CIFAR-10 Batch 2:  Loss: 0.0006718853837810457\n",
      "Accuracy: 0.7447998523712158\n",
      "Epoch 154, CIFAR-10 Batch 3:  Loss: 0.0004948439309373498\n",
      "Accuracy: 0.7405998706817627\n",
      "Epoch 154, CIFAR-10 Batch 4:  Loss: 0.005403028801083565\n",
      "Accuracy: 0.738399863243103\n",
      "Epoch 154, CIFAR-10 Batch 5:  Loss: 0.0009991818806156516\n",
      "Accuracy: 0.7503998279571533\n",
      "Epoch 155, CIFAR-10 Batch 1:  Loss: 0.0007636219379492104\n",
      "Accuracy: 0.7519998550415039\n",
      "Epoch 155, CIFAR-10 Batch 2:  Loss: 0.0002815798216033727\n",
      "Accuracy: 0.747799813747406\n",
      "Epoch 155, CIFAR-10 Batch 3:  Loss: 0.0015829098410904408\n",
      "Accuracy: 0.7385998964309692\n",
      "Epoch 155, CIFAR-10 Batch 4:  Loss: 0.018213380128145218\n",
      "Accuracy: 0.7453998327255249\n",
      "Epoch 155, CIFAR-10 Batch 5:  Loss: 0.003860424505546689\n",
      "Accuracy: 0.7493998408317566\n",
      "Epoch 156, CIFAR-10 Batch 1:  Loss: 0.0008313352009281516\n",
      "Accuracy: 0.749599814414978\n",
      "Epoch 156, CIFAR-10 Batch 2:  Loss: 0.0005725253722630441\n",
      "Accuracy: 0.7443998456001282\n",
      "Epoch 156, CIFAR-10 Batch 3:  Loss: 0.0015579622704535723\n",
      "Accuracy: 0.7453998327255249\n",
      "Epoch 156, CIFAR-10 Batch 4:  Loss: 0.0028947116807103157\n",
      "Accuracy: 0.7413997650146484\n",
      "Epoch 156, CIFAR-10 Batch 5:  Loss: 0.001267242943868041\n",
      "Accuracy: 0.7541998028755188\n",
      "Epoch 157, CIFAR-10 Batch 1:  Loss: 0.001512458547949791\n",
      "Accuracy: 0.7441998720169067\n",
      "Epoch 157, CIFAR-10 Batch 2:  Loss: 0.0020112134516239166\n",
      "Accuracy: 0.7427998781204224\n",
      "Epoch 157, CIFAR-10 Batch 3:  Loss: 0.002867639297619462\n",
      "Accuracy: 0.7293997406959534\n",
      "Epoch 157, CIFAR-10 Batch 4:  Loss: 0.0023139440454542637\n",
      "Accuracy: 0.739399790763855\n",
      "Epoch 157, CIFAR-10 Batch 5:  Loss: 0.00249921134673059\n",
      "Accuracy: 0.7475997805595398\n",
      "Epoch 158, CIFAR-10 Batch 1:  Loss: 0.0005377412308007479\n",
      "Accuracy: 0.7413997650146484\n",
      "Epoch 158, CIFAR-10 Batch 2:  Loss: 0.0005390087026171386\n",
      "Accuracy: 0.7433998584747314\n",
      "Epoch 158, CIFAR-10 Batch 3:  Loss: 0.0004355804994702339\n",
      "Accuracy: 0.7421998977661133\n",
      "Epoch 158, CIFAR-10 Batch 4:  Loss: 0.0014384854584932327\n",
      "Accuracy: 0.7455998063087463\n",
      "Epoch 158, CIFAR-10 Batch 5:  Loss: 0.0033633571583777666\n",
      "Accuracy: 0.742999792098999\n",
      "Epoch 159, CIFAR-10 Batch 1:  Loss: 0.0009547234512865543\n",
      "Accuracy: 0.7495998740196228\n",
      "Epoch 159, CIFAR-10 Batch 2:  Loss: 0.00041804055217653513\n",
      "Accuracy: 0.7363998889923096\n",
      "Epoch 159, CIFAR-10 Batch 3:  Loss: 0.0008479917305521667\n",
      "Accuracy: 0.7465998530387878\n",
      "Epoch 159, CIFAR-10 Batch 4:  Loss: 0.0006836568936705589\n",
      "Accuracy: 0.7399998307228088\n",
      "Epoch 159, CIFAR-10 Batch 5:  Loss: 0.0009618550539016724\n",
      "Accuracy: 0.7527998685836792\n",
      "Epoch 160, CIFAR-10 Batch 1:  Loss: 0.0010642854031175375\n",
      "Accuracy: 0.74319988489151\n",
      "Epoch 160, CIFAR-10 Batch 2:  Loss: 0.0009664679528214037\n",
      "Accuracy: 0.746799886226654\n",
      "Epoch 160, CIFAR-10 Batch 3:  Loss: 0.00036881244159303606\n",
      "Accuracy: 0.7507998943328857\n",
      "Epoch 160, CIFAR-10 Batch 4:  Loss: 0.0007293112576007843\n",
      "Accuracy: 0.7525998950004578\n",
      "Epoch 160, CIFAR-10 Batch 5:  Loss: 0.0011181709123775363\n",
      "Accuracy: 0.7473998069763184\n",
      "Epoch 161, CIFAR-10 Batch 1:  Loss: 0.0017397374613210559\n",
      "Accuracy: 0.7391997575759888\n",
      "Epoch 161, CIFAR-10 Batch 2:  Loss: 0.0017081818077713251\n",
      "Accuracy: 0.744999885559082\n",
      "Epoch 161, CIFAR-10 Batch 3:  Loss: 0.002064839005470276\n",
      "Accuracy: 0.7533998489379883\n",
      "Epoch 161, CIFAR-10 Batch 4:  Loss: 0.0020641046576201916\n",
      "Accuracy: 0.7379997968673706\n",
      "Epoch 161, CIFAR-10 Batch 5:  Loss: 0.003619533497840166\n",
      "Accuracy: 0.7419998049736023\n",
      "Epoch 162, CIFAR-10 Batch 1:  Loss: 0.0013044278603047132\n",
      "Accuracy: 0.7539998292922974\n",
      "Epoch 162, CIFAR-10 Batch 2:  Loss: 0.0007530765724368393\n",
      "Accuracy: 0.7435998320579529\n",
      "Epoch 162, CIFAR-10 Batch 3:  Loss: 0.0007120826048776507\n",
      "Accuracy: 0.7433997988700867\n",
      "Epoch 162, CIFAR-10 Batch 4:  Loss: 0.0004092963645234704\n",
      "Accuracy: 0.7451997995376587\n",
      "Epoch 162, CIFAR-10 Batch 5:  Loss: 0.0012157750315964222\n",
      "Accuracy: 0.7437998652458191\n",
      "Epoch 163, CIFAR-10 Batch 1:  Loss: 0.0030657483730465174\n",
      "Accuracy: 0.7521998286247253\n",
      "Epoch 163, CIFAR-10 Batch 2:  Loss: 0.0007062886143103242\n",
      "Accuracy: 0.7419998645782471\n",
      "Epoch 163, CIFAR-10 Batch 3:  Loss: 0.0012501898454502225\n",
      "Accuracy: 0.7437998056411743\n",
      "Epoch 163, CIFAR-10 Batch 4:  Loss: 0.0011954347137361765\n",
      "Accuracy: 0.7431998252868652\n",
      "Epoch 163, CIFAR-10 Batch 5:  Loss: 0.0013012270210310817\n",
      "Accuracy: 0.7447998523712158\n",
      "Epoch 164, CIFAR-10 Batch 1:  Loss: 0.0011190747609362006\n",
      "Accuracy: 0.7519998550415039\n",
      "Epoch 164, CIFAR-10 Batch 2:  Loss: 0.0012865050230175257\n",
      "Accuracy: 0.7457997798919678\n",
      "Epoch 164, CIFAR-10 Batch 3:  Loss: 0.002252962440252304\n",
      "Accuracy: 0.739599883556366\n",
      "Epoch 164, CIFAR-10 Batch 4:  Loss: 0.0008118581026792526\n",
      "Accuracy: 0.7499998211860657\n",
      "Epoch 164, CIFAR-10 Batch 5:  Loss: 0.0018432042561471462\n",
      "Accuracy: 0.7517998218536377\n",
      "Epoch 165, CIFAR-10 Batch 1:  Loss: 0.0012874483363702893\n",
      "Accuracy: 0.7483998537063599\n",
      "Epoch 165, CIFAR-10 Batch 2:  Loss: 0.0017982743447646499\n",
      "Accuracy: 0.7497998476028442\n",
      "Epoch 165, CIFAR-10 Batch 3:  Loss: 0.001809887820854783\n",
      "Accuracy: 0.739599883556366\n",
      "Epoch 165, CIFAR-10 Batch 4:  Loss: 0.0009466148330830038\n",
      "Accuracy: 0.7519997954368591\n",
      "Epoch 165, CIFAR-10 Batch 5:  Loss: 0.004469959530979395\n",
      "Accuracy: 0.7487998604774475\n",
      "Epoch 166, CIFAR-10 Batch 1:  Loss: 0.004806823562830687\n",
      "Accuracy: 0.7403998374938965\n",
      "Epoch 166, CIFAR-10 Batch 2:  Loss: 0.004458900075405836\n",
      "Accuracy: 0.7345998287200928\n",
      "Epoch 166, CIFAR-10 Batch 3:  Loss: 0.0031259246170520782\n",
      "Accuracy: 0.7509998083114624\n",
      "Epoch 166, CIFAR-10 Batch 4:  Loss: 0.0014611822552978992\n",
      "Accuracy: 0.7557997703552246\n",
      "Epoch 166, CIFAR-10 Batch 5:  Loss: 0.00232436484657228\n",
      "Accuracy: 0.744399905204773\n",
      "Epoch 167, CIFAR-10 Batch 1:  Loss: 0.00035352533450350165\n",
      "Accuracy: 0.7481998205184937\n",
      "Epoch 167, CIFAR-10 Batch 2:  Loss: 0.0020726739894598722\n",
      "Accuracy: 0.7449998259544373\n",
      "Epoch 167, CIFAR-10 Batch 3:  Loss: 0.005881817080080509\n",
      "Accuracy: 0.747799813747406\n",
      "Epoch 167, CIFAR-10 Batch 4:  Loss: 0.0002586264454293996\n",
      "Accuracy: 0.7533998489379883\n",
      "Epoch 167, CIFAR-10 Batch 5:  Loss: 0.0025760959833860397\n",
      "Accuracy: 0.7431998252868652\n",
      "Epoch 168, CIFAR-10 Batch 1:  Loss: 0.0014316249871626496\n",
      "Accuracy: 0.7487998008728027\n",
      "Epoch 168, CIFAR-10 Batch 2:  Loss: 0.022130433470010757\n",
      "Accuracy: 0.7277998328208923\n",
      "Epoch 168, CIFAR-10 Batch 3:  Loss: 0.002102110767737031\n",
      "Accuracy: 0.7437998056411743\n",
      "Epoch 168, CIFAR-10 Batch 4:  Loss: 0.001991408411413431\n",
      "Accuracy: 0.7433997988700867\n",
      "Epoch 168, CIFAR-10 Batch 5:  Loss: 0.0019331122748553753\n",
      "Accuracy: 0.7457998394966125\n",
      "Epoch 169, CIFAR-10 Batch 1:  Loss: 0.0075786784291267395\n",
      "Accuracy: 0.7423998117446899\n",
      "Epoch 169, CIFAR-10 Batch 2:  Loss: 0.0021360910031944513\n",
      "Accuracy: 0.7445998191833496\n",
      "Epoch 169, CIFAR-10 Batch 3:  Loss: 0.001151649048551917\n",
      "Accuracy: 0.7439998388290405\n",
      "Epoch 169, CIFAR-10 Batch 4:  Loss: 0.0006843384471721947\n",
      "Accuracy: 0.744999885559082\n",
      "Epoch 169, CIFAR-10 Batch 5:  Loss: 0.00520525174215436\n",
      "Accuracy: 0.7419998049736023\n",
      "Epoch 170, CIFAR-10 Batch 1:  Loss: 0.0009342022822238505\n",
      "Accuracy: 0.731799840927124\n",
      "Epoch 170, CIFAR-10 Batch 2:  Loss: 0.0028302157297730446\n",
      "Accuracy: 0.7361997961997986\n",
      "Epoch 170, CIFAR-10 Batch 3:  Loss: 0.002968590008094907\n",
      "Accuracy: 0.7407998442649841\n",
      "Epoch 170, CIFAR-10 Batch 4:  Loss: 0.0010889254044741392\n",
      "Accuracy: 0.7431998252868652\n",
      "Epoch 170, CIFAR-10 Batch 5:  Loss: 0.0016583249671384692\n",
      "Accuracy: 0.7409998774528503\n",
      "Epoch 171, CIFAR-10 Batch 1:  Loss: 0.00394692225381732\n",
      "Accuracy: 0.7549998164176941\n",
      "Epoch 171, CIFAR-10 Batch 2:  Loss: 0.00046607808326371014\n",
      "Accuracy: 0.7397998571395874\n",
      "Epoch 171, CIFAR-10 Batch 3:  Loss: 0.00061123474733904\n",
      "Accuracy: 0.7385998964309692\n",
      "Epoch 171, CIFAR-10 Batch 4:  Loss: 0.0006608373369090259\n",
      "Accuracy: 0.7377998232841492\n",
      "Epoch 171, CIFAR-10 Batch 5:  Loss: 0.0015032001538202167\n",
      "Accuracy: 0.7407998442649841\n",
      "Epoch 172, CIFAR-10 Batch 1:  Loss: 0.00045990507351234555\n",
      "Accuracy: 0.7411998510360718\n",
      "Epoch 172, CIFAR-10 Batch 2:  Loss: 0.0022388859651982784\n",
      "Accuracy: 0.7353999018669128\n",
      "Epoch 172, CIFAR-10 Batch 3:  Loss: 0.0021001966670155525\n",
      "Accuracy: 0.7509998679161072\n",
      "Epoch 172, CIFAR-10 Batch 4:  Loss: 0.009283104911446571\n",
      "Accuracy: 0.7373998165130615\n",
      "Epoch 172, CIFAR-10 Batch 5:  Loss: 0.0017518281238153577\n",
      "Accuracy: 0.738399863243103\n",
      "Epoch 173, CIFAR-10 Batch 1:  Loss: 0.002576020546257496\n",
      "Accuracy: 0.7399998307228088\n",
      "Epoch 173, CIFAR-10 Batch 2:  Loss: 0.005429815035313368\n",
      "Accuracy: 0.7379998564720154\n",
      "Epoch 173, CIFAR-10 Batch 3:  Loss: 0.0014203228056430817\n",
      "Accuracy: 0.7415999174118042\n",
      "Epoch 173, CIFAR-10 Batch 4:  Loss: 0.002265903865918517\n",
      "Accuracy: 0.7485998868942261\n",
      "Epoch 173, CIFAR-10 Batch 5:  Loss: 0.00893308874219656\n",
      "Accuracy: 0.7453998327255249\n",
      "Epoch 174, CIFAR-10 Batch 1:  Loss: 0.0002406866115052253\n",
      "Accuracy: 0.7427998781204224\n",
      "Epoch 174, CIFAR-10 Batch 2:  Loss: 0.0021915424149483442\n",
      "Accuracy: 0.7435998916625977\n",
      "Epoch 174, CIFAR-10 Batch 3:  Loss: 0.0012644273228943348\n",
      "Accuracy: 0.744999885559082\n",
      "Epoch 174, CIFAR-10 Batch 4:  Loss: 0.0009197269100695848\n",
      "Accuracy: 0.7435997724533081\n",
      "Epoch 174, CIFAR-10 Batch 5:  Loss: 0.0025976181495934725\n",
      "Accuracy: 0.7405998110771179\n",
      "Epoch 175, CIFAR-10 Batch 1:  Loss: 0.009870681911706924\n",
      "Accuracy: 0.721599817276001\n",
      "Epoch 175, CIFAR-10 Batch 2:  Loss: 0.010079238563776016\n",
      "Accuracy: 0.7301998138427734\n",
      "Epoch 175, CIFAR-10 Batch 3:  Loss: 0.0034142620861530304\n",
      "Accuracy: 0.7387998104095459\n",
      "Epoch 175, CIFAR-10 Batch 4:  Loss: 0.001215156284160912\n",
      "Accuracy: 0.7451998591423035\n",
      "Epoch 175, CIFAR-10 Batch 5:  Loss: 0.00074615707853809\n",
      "Accuracy: 0.7481998205184937\n",
      "Epoch 176, CIFAR-10 Batch 1:  Loss: 0.001512558781541884\n",
      "Accuracy: 0.7447998523712158\n",
      "Epoch 176, CIFAR-10 Batch 2:  Loss: 0.004206162877380848\n",
      "Accuracy: 0.7429998517036438\n",
      "Epoch 176, CIFAR-10 Batch 3:  Loss: 0.0008632688550278544\n",
      "Accuracy: 0.747799813747406\n",
      "Epoch 176, CIFAR-10 Batch 4:  Loss: 0.0005324733210727572\n",
      "Accuracy: 0.749599814414978\n",
      "Epoch 176, CIFAR-10 Batch 5:  Loss: 0.0002865782880689949\n",
      "Accuracy: 0.7469998002052307\n",
      "Epoch 177, CIFAR-10 Batch 1:  Loss: 0.0012150303227826953\n",
      "Accuracy: 0.7415999174118042\n",
      "Epoch 177, CIFAR-10 Batch 2:  Loss: 0.0012417415855452418\n",
      "Accuracy: 0.7457998394966125\n",
      "Epoch 177, CIFAR-10 Batch 3:  Loss: 0.0012894326355308294\n",
      "Accuracy: 0.7449998259544373\n",
      "Epoch 177, CIFAR-10 Batch 4:  Loss: 0.001172008691355586\n",
      "Accuracy: 0.7419998049736023\n",
      "Epoch 177, CIFAR-10 Batch 5:  Loss: 0.0018077365821227431\n",
      "Accuracy: 0.7389998435974121\n",
      "Epoch 178, CIFAR-10 Batch 1:  Loss: 0.0009963484480977058\n",
      "Accuracy: 0.7413998246192932\n",
      "Epoch 178, CIFAR-10 Batch 2:  Loss: 0.0009023702004924417\n",
      "Accuracy: 0.7465998530387878\n",
      "Epoch 178, CIFAR-10 Batch 3:  Loss: 0.004031053744256496\n",
      "Accuracy: 0.7483998537063599\n",
      "Epoch 178, CIFAR-10 Batch 4:  Loss: 0.0007747492054477334\n",
      "Accuracy: 0.7471997737884521\n",
      "Epoch 178, CIFAR-10 Batch 5:  Loss: 0.0006734197959303856\n",
      "Accuracy: 0.7485998272895813\n",
      "Epoch 179, CIFAR-10 Batch 1:  Loss: 0.0004487910773605108\n",
      "Accuracy: 0.7451997995376587\n",
      "Epoch 179, CIFAR-10 Batch 2:  Loss: 0.002187094185501337\n",
      "Accuracy: 0.7397998571395874\n",
      "Epoch 179, CIFAR-10 Batch 3:  Loss: 0.007566199172288179\n",
      "Accuracy: 0.7371997833251953\n",
      "Epoch 179, CIFAR-10 Batch 4:  Loss: 0.0009156037704087794\n",
      "Accuracy: 0.7423999309539795\n",
      "Epoch 179, CIFAR-10 Batch 5:  Loss: 0.0018203818472102284\n",
      "Accuracy: 0.7531998753547668\n",
      "Epoch 180, CIFAR-10 Batch 1:  Loss: 0.005521878600120544\n",
      "Accuracy: 0.74319988489151\n",
      "Epoch 180, CIFAR-10 Batch 2:  Loss: 0.0017488233279436827\n",
      "Accuracy: 0.7349998354911804\n",
      "Epoch 180, CIFAR-10 Batch 3:  Loss: 0.007306513376533985\n",
      "Accuracy: 0.7385998368263245\n",
      "Epoch 180, CIFAR-10 Batch 4:  Loss: 0.006840589921921492\n",
      "Accuracy: 0.7293998599052429\n",
      "Epoch 180, CIFAR-10 Batch 5:  Loss: 0.0006072842516005039\n",
      "Accuracy: 0.7455998659133911\n",
      "Epoch 181, CIFAR-10 Batch 1:  Loss: 0.0014776581665500998\n",
      "Accuracy: 0.7395998239517212\n",
      "Epoch 181, CIFAR-10 Batch 2:  Loss: 0.0026553855277597904\n",
      "Accuracy: 0.7441998720169067\n",
      "Epoch 181, CIFAR-10 Batch 3:  Loss: 0.002340980339795351\n",
      "Accuracy: 0.744399905204773\n",
      "Epoch 181, CIFAR-10 Batch 4:  Loss: 0.0011133943917229772\n",
      "Accuracy: 0.7487998604774475\n",
      "Epoch 181, CIFAR-10 Batch 5:  Loss: 0.0010595939820632339\n",
      "Accuracy: 0.7453998327255249\n",
      "Epoch 182, CIFAR-10 Batch 1:  Loss: 0.0010209293104708195\n",
      "Accuracy: 0.7459998726844788\n",
      "Epoch 182, CIFAR-10 Batch 2:  Loss: 0.0005831681773997843\n",
      "Accuracy: 0.7527998685836792\n",
      "Epoch 182, CIFAR-10 Batch 3:  Loss: 0.0018438964616507292\n",
      "Accuracy: 0.7455998659133911\n",
      "Epoch 182, CIFAR-10 Batch 4:  Loss: 0.004302613437175751\n",
      "Accuracy: 0.7395998239517212\n",
      "Epoch 182, CIFAR-10 Batch 5:  Loss: 0.0016990031581372023\n",
      "Accuracy: 0.7445998787879944\n",
      "Epoch 183, CIFAR-10 Batch 1:  Loss: 0.004144181963056326\n",
      "Accuracy: 0.7401998043060303\n",
      "Epoch 183, CIFAR-10 Batch 2:  Loss: 0.0014359992928802967\n",
      "Accuracy: 0.7421998381614685\n",
      "Epoch 183, CIFAR-10 Batch 3:  Loss: 0.00112002098467201\n",
      "Accuracy: 0.7487998604774475\n",
      "Epoch 183, CIFAR-10 Batch 4:  Loss: 0.000542724272236228\n",
      "Accuracy: 0.7445998191833496\n",
      "Epoch 183, CIFAR-10 Batch 5:  Loss: 0.004060591571033001\n",
      "Accuracy: 0.7409998178482056\n",
      "Epoch 184, CIFAR-10 Batch 1:  Loss: 0.002658815821632743\n",
      "Accuracy: 0.7491998672485352\n",
      "Epoch 184, CIFAR-10 Batch 2:  Loss: 0.001357505563646555\n",
      "Accuracy: 0.7395998239517212\n",
      "Epoch 184, CIFAR-10 Batch 3:  Loss: 0.0010557728819549084\n",
      "Accuracy: 0.7487998604774475\n",
      "Epoch 184, CIFAR-10 Batch 4:  Loss: 0.0008485849248245358\n",
      "Accuracy: 0.7511998414993286\n",
      "Epoch 184, CIFAR-10 Batch 5:  Loss: 0.0005722670466639102\n",
      "Accuracy: 0.7439998388290405\n",
      "Epoch 185, CIFAR-10 Batch 1:  Loss: 0.0005156092811375856\n",
      "Accuracy: 0.7503998279571533\n",
      "Epoch 185, CIFAR-10 Batch 2:  Loss: 0.0012245458783581853\n",
      "Accuracy: 0.7465997934341431\n",
      "Epoch 185, CIFAR-10 Batch 3:  Loss: 0.002601000014692545\n",
      "Accuracy: 0.7419998049736023\n",
      "Epoch 185, CIFAR-10 Batch 4:  Loss: 0.0014241884928196669\n",
      "Accuracy: 0.7469998598098755\n",
      "Epoch 185, CIFAR-10 Batch 5:  Loss: 0.0042397077195346355\n",
      "Accuracy: 0.7501998543739319\n",
      "Epoch 186, CIFAR-10 Batch 1:  Loss: 0.001385012990795076\n",
      "Accuracy: 0.7545998096466064\n",
      "Epoch 186, CIFAR-10 Batch 2:  Loss: 0.0006041043670848012\n",
      "Accuracy: 0.744199812412262\n",
      "Epoch 186, CIFAR-10 Batch 3:  Loss: 0.0005280021578073502\n",
      "Accuracy: 0.7395998239517212\n",
      "Epoch 186, CIFAR-10 Batch 4:  Loss: 0.002014336409047246\n",
      "Accuracy: 0.7437998056411743\n",
      "Epoch 186, CIFAR-10 Batch 5:  Loss: 0.0016342831077054143\n",
      "Accuracy: 0.748999834060669\n",
      "Epoch 187, CIFAR-10 Batch 1:  Loss: 0.0013287158217281103\n",
      "Accuracy: 0.7499998807907104\n",
      "Epoch 187, CIFAR-10 Batch 2:  Loss: 0.0014332628343254328\n",
      "Accuracy: 0.7327998876571655\n",
      "Epoch 187, CIFAR-10 Batch 3:  Loss: 0.00016251692431978881\n",
      "Accuracy: 0.7493998408317566\n",
      "Epoch 187, CIFAR-10 Batch 4:  Loss: 0.001667011994868517\n",
      "Accuracy: 0.7343998551368713\n",
      "Epoch 187, CIFAR-10 Batch 5:  Loss: 0.0005206703208386898\n",
      "Accuracy: 0.748999834060669\n",
      "Epoch 188, CIFAR-10 Batch 1:  Loss: 0.0011411037994548678\n",
      "Accuracy: 0.7399998307228088\n",
      "Epoch 188, CIFAR-10 Batch 2:  Loss: 0.0007745205075480044\n",
      "Accuracy: 0.7399998307228088\n",
      "Epoch 188, CIFAR-10 Batch 3:  Loss: 0.00226665404625237\n",
      "Accuracy: 0.737599790096283\n",
      "Epoch 188, CIFAR-10 Batch 4:  Loss: 0.000512219441588968\n",
      "Accuracy: 0.7423998117446899\n",
      "Epoch 188, CIFAR-10 Batch 5:  Loss: 0.0006065875058993697\n",
      "Accuracy: 0.7425998449325562\n",
      "Epoch 189, CIFAR-10 Batch 1:  Loss: 0.0009403047151863575\n",
      "Accuracy: 0.7401998043060303\n",
      "Epoch 189, CIFAR-10 Batch 2:  Loss: 0.0005511654308065772\n",
      "Accuracy: 0.7419998645782471\n",
      "Epoch 189, CIFAR-10 Batch 3:  Loss: 0.0005600276635959744\n",
      "Accuracy: 0.7433998584747314\n",
      "Epoch 189, CIFAR-10 Batch 4:  Loss: 0.0007244318258017302\n",
      "Accuracy: 0.7451998591423035\n",
      "Epoch 189, CIFAR-10 Batch 5:  Loss: 0.0010207706363871694\n",
      "Accuracy: 0.7475998401641846\n",
      "Epoch 190, CIFAR-10 Batch 1:  Loss: 0.00037262580008246005\n",
      "Accuracy: 0.750799834728241\n",
      "Epoch 190, CIFAR-10 Batch 2:  Loss: 0.0013011773116886616\n",
      "Accuracy: 0.7417998909950256\n",
      "Epoch 190, CIFAR-10 Batch 3:  Loss: 0.00029791801352985203\n",
      "Accuracy: 0.7361998558044434\n",
      "Epoch 190, CIFAR-10 Batch 4:  Loss: 0.0005504859145730734\n",
      "Accuracy: 0.7559998631477356\n",
      "Epoch 190, CIFAR-10 Batch 5:  Loss: 0.001422735396772623\n",
      "Accuracy: 0.7537997961044312\n",
      "Epoch 191, CIFAR-10 Batch 1:  Loss: 0.00035178332473151386\n",
      "Accuracy: 0.7509998679161072\n",
      "Epoch 191, CIFAR-10 Batch 2:  Loss: 0.0012086839415133\n",
      "Accuracy: 0.7389997839927673\n",
      "Epoch 191, CIFAR-10 Batch 3:  Loss: 0.0004712613590527326\n",
      "Accuracy: 0.7461998462677002\n",
      "Epoch 191, CIFAR-10 Batch 4:  Loss: 0.0016383279580622911\n",
      "Accuracy: 0.7413998246192932\n",
      "Epoch 191, CIFAR-10 Batch 5:  Loss: 0.0016094392631202936\n",
      "Accuracy: 0.7471998333930969\n",
      "Epoch 192, CIFAR-10 Batch 1:  Loss: 0.000493593281134963\n",
      "Accuracy: 0.7475997805595398\n",
      "Epoch 192, CIFAR-10 Batch 2:  Loss: 0.005848128814250231\n",
      "Accuracy: 0.7303998470306396\n",
      "Epoch 192, CIFAR-10 Batch 3:  Loss: 0.001972438534721732\n",
      "Accuracy: 0.7417998313903809\n",
      "Epoch 192, CIFAR-10 Batch 4:  Loss: 0.00032388922409154475\n",
      "Accuracy: 0.7481998205184937\n",
      "Epoch 192, CIFAR-10 Batch 5:  Loss: 0.0007096101762726903\n",
      "Accuracy: 0.7451997995376587\n",
      "Epoch 193, CIFAR-10 Batch 1:  Loss: 0.0036243039648979902\n",
      "Accuracy: 0.7407998442649841\n",
      "Epoch 193, CIFAR-10 Batch 2:  Loss: 0.0006268407450988889\n",
      "Accuracy: 0.7529998421669006\n",
      "Epoch 193, CIFAR-10 Batch 3:  Loss: 0.00041611131746321917\n",
      "Accuracy: 0.7431998252868652\n",
      "Epoch 193, CIFAR-10 Batch 4:  Loss: 0.0014671622775495052\n",
      "Accuracy: 0.734799861907959\n",
      "Epoch 193, CIFAR-10 Batch 5:  Loss: 0.0007557458593510091\n",
      "Accuracy: 0.7449997663497925\n",
      "Epoch 194, CIFAR-10 Batch 1:  Loss: 0.0022323746234178543\n",
      "Accuracy: 0.7415999174118042\n",
      "Epoch 194, CIFAR-10 Batch 2:  Loss: 0.001346222823485732\n",
      "Accuracy: 0.7357998490333557\n",
      "Epoch 194, CIFAR-10 Batch 3:  Loss: 0.00035835953895002604\n",
      "Accuracy: 0.7429998517036438\n",
      "Epoch 194, CIFAR-10 Batch 4:  Loss: 0.00037732100463472307\n",
      "Accuracy: 0.7485998272895813\n",
      "Epoch 194, CIFAR-10 Batch 5:  Loss: 0.00044021039502695203\n",
      "Accuracy: 0.7423997521400452\n",
      "Epoch 195, CIFAR-10 Batch 1:  Loss: 0.0003243280516471714\n",
      "Accuracy: 0.7369998097419739\n",
      "Epoch 195, CIFAR-10 Batch 2:  Loss: 0.0008357710321433842\n",
      "Accuracy: 0.7387998104095459\n",
      "Epoch 195, CIFAR-10 Batch 3:  Loss: 0.0015639662742614746\n",
      "Accuracy: 0.744399905204773\n",
      "Epoch 195, CIFAR-10 Batch 4:  Loss: 0.002526227617636323\n",
      "Accuracy: 0.7367998361587524\n",
      "Epoch 195, CIFAR-10 Batch 5:  Loss: 0.001879836549051106\n",
      "Accuracy: 0.7351998686790466\n",
      "Epoch 196, CIFAR-10 Batch 1:  Loss: 0.009411164559423923\n",
      "Accuracy: 0.7351998686790466\n",
      "Epoch 196, CIFAR-10 Batch 2:  Loss: 0.000349166220985353\n",
      "Accuracy: 0.7479998469352722\n",
      "Epoch 196, CIFAR-10 Batch 3:  Loss: 0.00023333272838499397\n",
      "Accuracy: 0.7535998821258545\n",
      "Epoch 196, CIFAR-10 Batch 4:  Loss: 0.00020015580230392516\n",
      "Accuracy: 0.7501998543739319\n",
      "Epoch 196, CIFAR-10 Batch 5:  Loss: 0.012786325998604298\n",
      "Accuracy: 0.7279999256134033\n",
      "Epoch 197, CIFAR-10 Batch 1:  Loss: 0.0016049720579758286\n",
      "Accuracy: 0.7447998523712158\n",
      "Epoch 197, CIFAR-10 Batch 2:  Loss: 0.0018472736701369286\n",
      "Accuracy: 0.7409998178482056\n",
      "Epoch 197, CIFAR-10 Batch 3:  Loss: 0.002252778736874461\n",
      "Accuracy: 0.7377998232841492\n",
      "Epoch 197, CIFAR-10 Batch 4:  Loss: 0.0019598824437707663\n",
      "Accuracy: 0.7385998964309692\n",
      "Epoch 197, CIFAR-10 Batch 5:  Loss: 0.0009039845317602158\n",
      "Accuracy: 0.7463998794555664\n",
      "Epoch 198, CIFAR-10 Batch 1:  Loss: 0.0020295141730457544\n",
      "Accuracy: 0.7479998469352722\n",
      "Epoch 198, CIFAR-10 Batch 2:  Loss: 0.0004410666588228196\n",
      "Accuracy: 0.7445998787879944\n",
      "Epoch 198, CIFAR-10 Batch 3:  Loss: 0.0003016939153894782\n",
      "Accuracy: 0.7475998401641846\n",
      "Epoch 198, CIFAR-10 Batch 4:  Loss: 0.007199487648904324\n",
      "Accuracy: 0.7385998368263245\n",
      "Epoch 198, CIFAR-10 Batch 5:  Loss: 0.0008860514499247074\n",
      "Accuracy: 0.744199812412262\n",
      "Epoch 199, CIFAR-10 Batch 1:  Loss: 0.001075314125046134\n",
      "Accuracy: 0.7421998381614685\n",
      "Epoch 199, CIFAR-10 Batch 2:  Loss: 0.0009370282059535384\n",
      "Accuracy: 0.7443998456001282\n",
      "Epoch 199, CIFAR-10 Batch 3:  Loss: 0.0010403477353975177\n",
      "Accuracy: 0.7475998401641846\n",
      "Epoch 199, CIFAR-10 Batch 4:  Loss: 0.00024543306790292263\n",
      "Accuracy: 0.7485998868942261\n",
      "Epoch 199, CIFAR-10 Batch 5:  Loss: 0.0005857974174432456\n",
      "Accuracy: 0.7319998145103455\n",
      "Epoch 200, CIFAR-10 Batch 1:  Loss: 0.003105743555352092\n",
      "Accuracy: 0.7457998991012573\n",
      "Epoch 200, CIFAR-10 Batch 2:  Loss: 0.00043186460970900953\n",
      "Accuracy: 0.737799882888794\n",
      "Epoch 200, CIFAR-10 Batch 3:  Loss: 0.003011347260326147\n",
      "Accuracy: 0.744799792766571\n",
      "Epoch 200, CIFAR-10 Batch 4:  Loss: 0.0013673988869413733\n",
      "Accuracy: 0.7415997982025146\n",
      "Epoch 200, CIFAR-10 Batch 5:  Loss: 0.0001399764878442511\n",
      "Accuracy: 0.7511997818946838\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "save_model_path = './image_classification'\n",
    "\n",
    "print('Training...')\n",
    "with tf.Session() as sess:\n",
    "    # Initializing the variables\n",
    "    sess.run(tf.global_variables_initializer())\n",
    "    \n",
    "    # Training cycle\n",
    "    for epoch in range(epochs):\n",
    "        # Loop over all batches\n",
    "        n_batches = 5\n",
    "        for batch_i in range(1, n_batches + 1):\n",
    "            for batch_features, batch_labels in helper.load_preprocess_training_batch(batch_i, batch_size):\n",
    "                train_neural_network(sess, optimizer, keep_probability, batch_features, batch_labels)\n",
    "            print('Epoch {:>2}, CIFAR-10 Batch {}:  '.format(epoch + 1, batch_i), end='')\n",
    "            print_stats(sess, batch_features, batch_labels, cost, accuracy)\n",
    "            \n",
    "    # Save Model\n",
    "    saver = tf.train.Saver()\n",
    "    save_path = saver.save(sess, save_model_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Checkpoint\n",
    "The model has been saved to disk.\n",
    "## Test Model\n",
    "Test your model against the test dataset.  This will be your final accuracy. You should have an accuracy greater than 50%. If you don't, keep tweaking the model architecture and parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testing Accuracy: 0.7440487146377563\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAsEAAAJ/CAYAAACZcQ5xAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\nIiIiUnXUCRYRERGRqvP/AeM73u89X6zHAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1385c7dd8d0>"
      ]
     },
     "metadata": {
      "image/png": {
       "height": 319,
       "width": 352
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "\"\"\"\n",
    "DON'T MODIFY ANYTHING IN THIS CELL\n",
    "\"\"\"\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "import tensorflow as tf\n",
    "import pickle\n",
    "import helper\n",
    "import random\n",
    "\n",
    "# Set batch size if not already set\n",
    "try:\n",
    "    if batch_size:\n",
    "        pass\n",
    "except NameError:\n",
    "    batch_size = 64\n",
    "\n",
    "save_model_path = './image_classification'\n",
    "n_samples = 4\n",
    "top_n_predictions = 3\n",
    "\n",
    "def test_model():\n",
    "    \"\"\"\n",
    "    Test the saved model against the test dataset\n",
    "    \"\"\"\n",
    "\n",
    "    test_features, test_labels = pickle.load(open('preprocess_training.p', mode='rb'))\n",
    "    loaded_graph = tf.Graph()\n",
    "\n",
    "    with tf.Session(graph=loaded_graph) as sess:\n",
    "        # Load model\n",
    "        loader = tf.train.import_meta_graph(save_model_path + '.meta')\n",
    "        loader.restore(sess, save_model_path)\n",
    "\n",
    "        # Get Tensors from loaded model\n",
    "        loaded_x = loaded_graph.get_tensor_by_name('x:0')\n",
    "        loaded_y = loaded_graph.get_tensor_by_name('y:0')\n",
    "        loaded_keep_prob = loaded_graph.get_tensor_by_name('keep_prob:0')\n",
    "        loaded_logits = loaded_graph.get_tensor_by_name('logits:0')\n",
    "        loaded_acc = loaded_graph.get_tensor_by_name('accuracy:0')\n",
    "        \n",
    "        # Get accuracy in batches for memory limitations\n",
    "        test_batch_acc_total = 0\n",
    "        test_batch_count = 0\n",
    "        \n",
    "        for train_feature_batch, train_label_batch in helper.batch_features_labels(test_features, test_labels, batch_size):\n",
    "            test_batch_acc_total += sess.run(\n",
    "                loaded_acc,\n",
    "                feed_dict={loaded_x: train_feature_batch, loaded_y: train_label_batch, loaded_keep_prob: 1.0})\n",
    "            test_batch_count += 1\n",
    "\n",
    "        print('Testing Accuracy: {}\\n'.format(test_batch_acc_total/test_batch_count))\n",
    "\n",
    "        # Print Random Samples\n",
    "        random_test_features, random_test_labels = tuple(zip(*random.sample(list(zip(test_features, test_labels)), n_samples)))\n",
    "        random_test_predictions = sess.run(\n",
    "            tf.nn.top_k(tf.nn.softmax(loaded_logits), top_n_predictions),\n",
    "            feed_dict={loaded_x: random_test_features, loaded_y: random_test_labels, loaded_keep_prob: 1.0})\n",
    "        helper.display_image_predictions(random_test_features, random_test_labels, random_test_predictions)\n",
    "\n",
    "\n",
    "test_model()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why 50-70% Accuracy?\n",
    "You might be wondering why you can't get an accuracy any higher. First things first, 50% isn't bad for a simple CNN.  Pure guessing would get you 10% accuracy. However, you might notice people are getting scores [well above 70%](http://rodrigob.github.io/are_we_there_yet/build/classification_datasets_results.html#43494641522d3130).  That's because we haven't taught you all there is to know about neural networks. We still need to cover a few more techniques.\n",
    "## Submitting This Project\n",
    "When submitting this project, make sure to run all the cells before saving the notebook.  Save the notebook file as \"dlnd_image_classification.ipynb\" and save it as a HTML file under \"File\" -> \"Download as\".  Include the \"helper.py\" and \"problem_unittests.py\" files in your submission."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
